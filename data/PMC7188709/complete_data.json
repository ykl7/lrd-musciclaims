{
  "paper_id": "PMC7188709",
  "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7188709/",
  "figures": {
    "figure_1": {
      "figure_number": "Figure 1",
      "title": "Fig. 1.",
      "caption": "Workflow for construction, training and testing of DeepSurvNet using H&E-stained histopathological images of brain tumours available at TCGA (https://portal.gdc.cancer.gov/projects). The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on this",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig1",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_1.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/666b259a0805/11517_2020_2147_Fig1_HTML.jpg"
    },
    "figure_2": {
      "figure_number": "Figure 2",
      "title": "Fig. 2.",
      "caption": "Simple CNN structure with fully connected layers",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig2",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_2.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/ec5d3daa24e3/11517_2020_2147_Fig2_HTML.jpg"
    },
    "figure_3": {
      "figure_number": "Figure 3",
      "title": "Fig. 3.",
      "caption": "a Accuracy curves and b loss curves for five classifiers in training phase. Left, patches 256 × 256; middle, patches 512 × 512; and right, patches 1024 × 1024",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig3",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_3.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/e59b18ca1e23/11517_2020_2147_Fig3_HTML.jpg"
    },
    "figure_4": {
      "figure_number": "Figure 4",
      "title": "Fig. 4.",
      "caption": "Five classifiers output on patches 256 × 256, confusion matrix in the left side and the area under ROC in right side for a VGG19, b MobileNet V2, c ResNet50, d) Inception V3 and e GoogleNet",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig4",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_4.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/b26046e351d4/11517_2020_2147_Fig4_HTML.jpg"
    },
    "figure_6": {
      "figure_number": "Figure 6",
      "title": "Fig. 6.",
      "caption": "a Confusion matrix for total patches, b the area under ROC curve for all patches in four classes and c DeepSurvNet outputs summary for all patches",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig6",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_6.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/3860b5fbcda4/11517_2020_2147_Fig6_HTML.jpg"
    },
    "figure_7": {
      "figure_number": "Figure 7",
      "title": "Fig. 7.",
      "caption": "Brain cancer–mutated genes expression analysis in the four survival classes. a 20 most effective mutated genes in brain cancer, b number of patients related to each mutated genes in each class, c recognition of the most important gene in each class based on Z-score analysis and d differences in frequency of mutations of each class with respect the frequency of mutations in class IV",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "Fig7",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7188709/images/figure_7.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/f626/7188709/a7bcfecc7ef9/11517_2020_2147_Fig7_HTML.jpg"
    }
  },
  "claims": [
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The experimental results illustrate that DeepSurvNet model is a distinguished classifier and open a new horizon in the field of survival analysis.Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "All the networks were implemented in python with the Keras [52], a high-level neural networks API running on Tensorflow framework [53], and trained using four NVIDIA 1080Ti GPUs.Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Methods\nConstruction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Construction, training and testing of DeepSurvNet\nFigure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The accuracy of all the classifier models is dependent on the image preprocessing steps b to d on thisDatasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We used this dataset for an independent test and to monitor the efficiency of our model (i.e. this data was not used for training of the model, for which only TCGA datasets were used).Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "This database is directly related to all the extracted ROIs used in our work and is available from the authors upon request.Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, the number of classes and ROIs in each one is sufficiently large for training the DCNN classifiers which are known to be extremely data hungry throughout the training phase [40].Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Also, μ and σ are the average and standard deviation of all values in the original image patch.Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "More performant patterns have also been developed such as residual layers which utilize skip connections introduced in ResNet50 [43].Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Figure 1 presents the steps (a to h) involved in the construction, training and testing of DeepSurvNet, which are described below.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Datasets used for training, testing and validation of deep learning classifiers (Fig. 1a)\nWe considered two different datasets for the classification of survival rates in patients who suffered from different types of brain cancer including glioblastoma multiform, mixed glioma, oligodendroglioma and astrocytoma.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel a",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Patients’ database creation: removing outliers and extraction of tumour regions of interest (ROIs) from WSIs (Fig. 1b)\n937 WSIs from 490 brain cancer patients were downloaded from TCGA.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel b",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Definition of different classes for survival (Fig. 1c)\nFor classification, we have considered 4 classes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel c",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Patch extraction from ROIs and patch standardization (Fig. 1d)\nROIs allocated to each class are large in size, and processing them directly is computationally demanding.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel d",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Training, validating and testing datasets and DCNN-based classifiers (Fig. 1e, f)\nFor each specific patch size extracted from TCGA dataset, we have divided all the patches into three different cohorts including training (80%), validating (18%) and testing (2%).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel e",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "An example of an early CNN structure can be seen in Fig. 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Five DCNN-based classifiers for brain cancer survival rate classification (Fig. 1g)\nIn order to classify different classes of survival rates based on different sizes of patches, we have considered the most popular DCNN classifiers in image recognition task including VGG19 [44], GoogleNet [45], ResNet50 [43], InceptionV3 [46] and MobileNetV2 [42].",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel g",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Since their classifier has good functionality on benchmarks like ILSVRC, we have included it as a survival rate classifier for this study.DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "DeepSurvNet classifier model (Fig. 1h)\nAfter the utilization of five classifiers introduced in the previous part on the different patch sizes, the best classifier model of survival rate is selected.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "Panel h",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Results and discussion\nSurvival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Survival rate classifiers comparison\nFigure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 3 shows training accuracy and loss curves in training phase for different patch sizes (256 × 256, 512 × 512 and 1024 × 1024) for all survival classifiers (note that all classifiers were applied to the same TCGA training patches).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Figure 4 shows the application of the 5 classifiers on 256 × 256 patch size.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Figure 5 shows the summary of the results.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "First, H&E histopathological images from each patient (Fig. 5a, b) were analysed in consultation with the clinical pathologist for the distinction of those regions that correspond to the tumour.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel a",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "These ROIs were used to extract 20 patches per patient for “patch classification” using the TCGA-trained DeepSurvNet classifier (Fig. 5b).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "Panel b",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "From the different patients, we observe that the frequency of class prediction per patient was highly biased towards a single class as would be expected since patches were derived from the same pathological sample (Fig. 5c, d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel c",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Remarkably, this single class perfectly matches the real class to which patients belong (9 of 9 patients, Fig. 5d).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "Panel d",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Confusion matrix results (Fig. 6) show that the application of DeepSurvNet to this unseen dataset led to an average global precision of 80%.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we analysed the distribution of frequency for mutated genes in the different survival classes using data derived from the TCGA database (Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "First, we found that by pooling all brain cancer data, the most highly mutated genes were PTEN, TTN, TP53EGFR, PLG and MUC 16 (Fig. 7a).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel a",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We then analysed the frequency of mutations within each class and compared it to the distributions for all patients (Fig. 7b).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel b",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To gain further insight into this, we performed a Z-score analysis to test whether there are highly mutated genes associated to each class by identifying those genes whose frequency of mutations is higher than 2 standard deviations of the frequency values for the entire set of genes (Fig. 7c).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel c",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For this, we calculated the differences in frequency of mutations of each class with respect to the frequency of mutations in class IV, to discover which genes are more often aberrant in those short survival cancers (compared to those with long survival) (Fig. 7d).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel d",
          "figure_key": "figure_7"
        }
      ]
    }
  ],
  "extraction_stats": {
    "figures_count": 6,
    "claims_count": 234,
    "images_downloaded": 6,
    "tables_filtered": 2
  }
}