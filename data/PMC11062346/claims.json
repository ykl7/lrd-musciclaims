[
  {
    "sentence": "There are several applications where the two-dimensional gaze position mapping is useful (Fig. 1, column 2).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "To that end, as designed, the algorithm provides the researcher with a 3D eye model necessary for the estimation of rotational eye angles relative to the volume in the scene camera image that can be used to calculate eye movements relative to objects in this volume and each other, such as vergence angles (Fig. 1, column 3).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "With the addition of an external reference sensor (such as an inertial measurement unit) these data can be further processed to assess eye movements relative to head movement (such as the angular vestibuloocular reflex, aVOR) and to the external world, such as saccadic or smooth gaze shifts (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "While there are experiments that rely on all iterations of the eye movement data stream, each data stream relies on a particular set of assumptions and requires additional processing steps (Fig. 1) that may add measurement uncertainty that is not assessed in the standard calibration accuracy testing protocols.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "To that end, the goal of this paper is to provide experimenters with a roadmap of the potential pitfalls that may affect eye movement data along the processing pipeline illustrated in Fig. 1, and how these can be ascertained for a given experimental setup/protocol.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "In this work, we focus on the sources of noise in the following domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the physical rotation of the eye in scene camera 3D space – a representation essential for understanding eye movements relative to the objects in the scene and each other (Fig. 1, column 3); and (3) the external projection of the estimated gaze point location onto the target plane or in relation to world coordinates (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Currently, to understand the error in each recording, the Pupil Capture software provides the researcher with qualitative visual feedback of eye position accuracy in the scene camera image (Fig. 1, column 2).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "An average accuracy estimate of eye rotation relative to the target is also available, but only if additional validation is performed (Fig. 1, column 3).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "These latter experiments address the assumption of tracker stability relative to the head during pupil detection and eye model estimation in the eye camera images (Fig. 1, column 1).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "However, eye model estimation (Fig. 1, column 2) relies not only on accurate pupil detection but also requires that there is no camera movement between the three (two eye and one scene) cameras.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Thus, in Study 1, we investigate the potential for movement between the three cameras as a source of error that may be introduced early in the eye-tracking pipeline (Study 1 in Fig. 2A).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "We find that depending on the type of natural behavior (Fig. 2B), substantial eye camera motion is possible independent of the scene camera and may need to be considered in experiments depending on the degree of head/body movement required.",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel B",
        "figure_key": "figure_2"
      },
      {
        "figure_number": "Figure 2",
        "panel": "Panel B",
        "figure_key": "figure_2"
      },
      {
        "figure_number": "Figure 2",
        "panel": "Panel B",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "In Study 2, we investigate sources of computational noise that can affect the two- and three-dimensional eye position and orientation estimates (Study 2 in Fig. 2A).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "First, we identify potential errors in gaze estimation in the 2D scene camera and how they might be affected by different calibration routines (Fig. 2C).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      },
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      },
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Experimental protocol: Eye cameras mobility evaluation\nTo evaluate the effect of eye camera mobility, two sets of experiments were performed with four tasks each (see Fig. 2, Study 1).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "In one set of experiments, the eye tracker recorded the human participants’ pupils, while in the static eye experiment, the eye tracker was recording printed images of eyes placed on a mask worn by the participants (shown in Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "To evaluate the effect of eye camera mobility, two sets of experiments were performed with four tasks each (see Fig. 2, Study 1).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "In one set of experiments, the eye tracker recorded the human participants’ pupils, while in the static eye experiment, the eye tracker was recording printed images of eyes placed on a mask worn by the participants (shown in Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "Equipment\nParticipants were equipped with the Pupil Core headset secured via adjustable, ratcheting headgear (Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "All cameras are placed on a lightweight, head-mounted frame (22.75 g, Pupil Labs, 2022) and can be moved to adjust for better capture of the pupils or the scene of interest (Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "A wired inertial measurement unit (IMU, LPMS-CURS2, LPMS Research, Tokyo, Japan) with 9 degrees of freedom (three accelerometer, three gyroscope, and three magnetometer axes) was firmly attached to the eye tracker frame using a custom mount (total IMU + enclosure weight = 14.17 g, Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "Participants were equipped with the Pupil Core headset secured via adjustable, ratcheting headgear (Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "All cameras are placed on a lightweight, head-mounted frame (22.75 g, Pupil Labs, 2022) and can be moved to adjust for better capture of the pupils or the scene of interest (Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "A wired inertial measurement unit (IMU, LPMS-CURS2, LPMS Research, Tokyo, Japan) with 9 degrees of freedom (three accelerometer, three gyroscope, and three magnetometer axes) was firmly attached to the eye tracker frame using a custom mount (total IMU + enclosure weight = 14.17 g, Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "Four tasks were performed (see Fig. 2, Study 2).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The participant was asked to fixate a succession of four sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers shown sequentially in different grid patterns (Fig. 2C, Calibration).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Second, a grid fixation task with 25 marker positions (Fig. 2C, Validation) was performed in a similar manner and used for validation (test set).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "In other words, the participant was asked to make 10° amplitude saccades (third task) and 4° amplitude saccades (fourth task), respectively (Fig. 2A, Saccades).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The experiment was also performed with the head unrestrained (Fig. 2A, Head Unrestrained), and the participant having been asked to keep it as still as possible during the grid fixation and saccade tasks.",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The central marker’s frame-by-frame position in the scene camera image during the aVOR task is presented in (Fig. 2A, aVOR).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Four tasks were performed (see Fig. 2, Study 2).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The participant was asked to fixate a succession of four sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers shown sequentially in different grid patterns (Fig. 2C, Calibration).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Second, a grid fixation task with 25 marker positions (Fig. 2C, Validation) was performed in a similar manner and used for validation (test set).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "In other words, the participant was asked to make 10° amplitude saccades (third task) and 4° amplitude saccades (fourth task), respectively (Fig. 2A, Saccades).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The experiment was also performed with the head unrestrained (Fig. 2A, Head Unrestrained), and the participant having been asked to keep it as still as possible during the grid fixation and saccade tasks.",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "The central marker’s frame-by-frame position in the scene camera image during the aVOR task is presented in (Fig. 2A, aVOR).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "An IMU was attached to the headset as in Study 1 and aligned with the scene camera such that the coordinate frames of both sensors were parallel and the axes aligned in the same direction [Fig. 3, also see (Velisar & Shanidze, 2021)].",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "An IMU was attached to the headset as in Study 1 and aligned with the scene camera such that the coordinate frames of both sensors were parallel and the axes aligned in the same direction [Fig. 3, also see (Velisar & Shanidze, 2021)].",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "The measurement error produced by the five calibration choreographies evaluated in this study was measured in three different spaces: (1) the 2D scene camera image in which a projection of the gaze point was compared with the respective reference point (Fig. 1, column 2); (2) the eye rotation angles between successive fixation positions, the vergence angles and the gaze point depth estimates in the 3D scene camera space (Fig. 1, column 3); and (3) the eye positions on the screen and eye rotations relative to the head in the world-centered coordinate frame (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "The Euclidian distance between the gaze point and target was calculated to estimate the measurement error in meters on the screen (see Fig. 9 in Results for reference) for the validation set and the saccade amplitude, the Euclidian distance between successive saccades starting positions.",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "The Euclidian distance between the gaze point and target was calculated to estimate the measurement error in meters on the screen (see Fig. 9 in Results for reference) for the validation set and the saccade amplitude, the Euclidian distance between successive saccades starting positions.",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "We used this process to calculate each calibration’s accuracy and precision over the validation set (Fig. 2C, Validation).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head Restrained\n\nP2 Head Unrestrained\n\nP1 with EyeLink\n\nP2 with EyeLink\n\n\n\n\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\n\n\n\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\n\n\n\n\n\n\n\n5 pts\n2.0 [1.4 2.4]\n\n0.9\n\n\n2.5 ± 1.1\n\n\n0.5\n\n1.5 [1.0 1.8]\n\n0.3\n\n1.4 [0.8 2.6]\n\n0.2\n\n\n\n9 pts\n\n1.8 ± 1.0\n\n\n0.8\n\n\n2.6 ± 1.0\n\n\n0.4\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.9 [1.2 2.5]\n\n0.2\n\n\n\n13 pts\n\n1.8 ± 0.9\n\n\n0.8\n\n1.9 [1.4 2.8]\n\n0.5\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.5 [1.1 2.7]\n\n0.2\n\n\n\nStar\n1.5 [1.1 2.5]\n\n0.4\n\n2.6 [1.1 4.7]\n\n0.5\n\n1.0 [0.5 2.1]\n\n0.1\n\n1.6 [1.1 3.3]\n\n0.4\n\n\n\naVOR\n\n\n\n2.9 ± 0.9\n\n\n0.4\n\n\n\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*All accuracy calculations were done for the 25 validations points.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "We used this process to calculate each calibration’s accuracy and precision over the validation set (Fig. 2C, Validation).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head Restrained\n\nP2 Head Unrestrained\n\nP1 with EyeLink\n\nP2 with EyeLink\n\n\n\n\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\n\n\n\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\n\n\n\n\n\n\n\n5 pts\n2.0 [1.4 2.4]\n\n0.9\n\n\n2.5 ± 1.1\n\n\n0.5\n\n1.5 [1.0 1.8]\n\n0.3\n\n1.4 [0.8 2.6]\n\n0.2\n\n\n\n9 pts\n\n1.8 ± 1.0\n\n\n0.8\n\n\n2.6 ± 1.0\n\n\n0.4\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.9 [1.2 2.5]\n\n0.2\n\n\n\n13 pts\n\n1.8 ± 0.9\n\n\n0.8\n\n1.9 [1.4 2.8]\n\n0.5\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.5 [1.1 2.7]\n\n0.2\n\n\n\nStar\n1.5 [1.1 2.5]\n\n0.4\n\n2.6 [1.1 4.7]\n\n0.5\n\n1.0 [0.5 2.1]\n\n0.1\n\n1.6 [1.1 3.3]\n\n0.4\n\n\n\naVOR\n\n\n\n2.9 ± 0.9\n\n\n0.4\n\n\n\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*All accuracy calculations were done for the 25 validations points.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel A",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel A",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel A",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "To determine whether differences in the arrangement of calibration targets affect gaze estimation, we performed four head-restrained calibration choreographies: 5, 9, and 13 point full-screen grids and a nine-point central cross (star, Fig. 2C) and five head-unrestrained choreographies: the four used in the head-restrained calibrations and a single marker head movement calibration (aVOR, Fig. 2A, C).",
    "figure_references": [
      {
        "figure_number": "Figure 2",
        "panel": "Panel C",
        "figure_key": "figure_2"
      },
      {
        "figure_number": "Figure 2",
        "panel": "Panel A",
        "figure_key": "figure_2"
      }
    ]
  },
  {
    "sentence": "Furthermore, it is not clear if the same calibration approach would be the best choice for experiments in each of the reference frames described in Fig. 1.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "However, these have largely been reported only relative to the scene camera image (Fig. 1, column 3) and for the combined gaze point across the two eyes.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Further, we examine how the choice of calibration might affect these estimates in each analysis domain: (1) pixel distance in the scene camera image (Fig. 1, column 2), (2) as the angular difference between the binocular gaze point and the unprojected target locations in the scene camera space (Fig. 1, column 3), and (3) distance (meters) on the physical screen (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "The largest variability was observed for the head-unrestrained data, calibrated using the star calibration (Fig. 6C, red unfilled box).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel C",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "Figure 6A shows the reference points and an example set of gaze point locations in the 2D scene camera image for gaze calibrated with the nine-point calibration choreography and evaluated over the validation set (P2, simultaneous EyeLink session).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel A",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "Figure 6B shows the data set in Fig. 6A mapped on the display screen (red Xs).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel B",
        "figure_key": "figure_6"
      },
      {
        "figure_number": "Figure 6",
        "panel": "Panel A",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "By using the same pixel per degree conversion used by the EyeLink, the data can be represented in degrees of visual angle (shown in Fig. 6D).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel D",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "As can be observed in Fig. 6D, the errors measured by Pupil Core are larger than those from the EyeLink’s for all data sets and all calibrations.",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel D",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "The averaged errors shown here are comparable with the values reported by the Pupil Core software (Table 2) and the average errors calculated using the 3D eye rotations (shown in Fig. 7).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "The largest variability was observed for the head-unrestrained data, calibrated using the star calibration (Fig. 6C, red unfilled box).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel C",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "Figure 6A shows the reference points and an example set of gaze point locations in the 2D scene camera image for gaze calibrated with the nine-point calibration choreography and evaluated over the validation set (P2, simultaneous EyeLink session).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel A",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "Figure 6B shows the data set in Fig. 6A mapped on the display screen (red Xs).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel B",
        "figure_key": "figure_6"
      },
      {
        "figure_number": "Figure 6",
        "panel": "Panel A",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "By using the same pixel per degree conversion used by the EyeLink, the data can be represented in degrees of visual angle (shown in Fig. 6D).",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel D",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "As can be observed in Fig. 6D, the errors measured by Pupil Core are larger than those from the EyeLink’s for all data sets and all calibrations.",
    "figure_references": [
      {
        "figure_number": "Figure 6",
        "panel": "Panel D",
        "figure_key": "figure_6"
      }
    ]
  },
  {
    "sentence": "The averaged errors shown here are comparable with the values reported by the Pupil Core software (Table 2) and the average errors calculated using the 3D eye rotations (shown in Fig. 7).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Pupil capture accuracy visualizer\nWe report the accuracy and precision estimates (Fig. 1, column 3) given by the Pupil software (Pupil Labs’ accuracy visualizer plug-in) for gaze points calculated for the validation set using each calibration (see Methods “Measured error in 2D: scene camera image and display screen“ section).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "We report the accuracy and precision estimates (Fig. 1, column 3) given by the Pupil software (Pupil Labs’ accuracy visualizer plug-in) for gaze points calculated for the validation set using each calibration (see Methods “Measured error in 2D: scene camera image and display screen“ section).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Gaze estimation accuracy in the 3D camera space\nTo calculate the accuracy (or more precisely the angle error, between the gaze point position vector and the respective unprojected reference point in the scene camera coordinate frame, Fig. 1, column 3) for all validation target fixations we implemented a similar algorithm to that above.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Unlike Pupil Core’s own single value estimate, this approach allowed us to estimate error at each spatial location of targets on screen, for each calibration (for example, see Fig. 7B, P2, head-restrained experiment, filled bars in Fig. 7A).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel B",
        "figure_key": "figure_7"
      },
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Using these individual points, we calculated the summary statistics shown in Fig. 7A and Table 3.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Similar error distribution was observed for the head-unrestrained condition (open bars in Fig. 7A).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "To calculate the accuracy (or more precisely the angle error, between the gaze point position vector and the respective unprojected reference point in the scene camera coordinate frame, Fig. 1, column 3) for all validation target fixations we implemented a similar algorithm to that above.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Unlike Pupil Core’s own single value estimate, this approach allowed us to estimate error at each spatial location of targets on screen, for each calibration (for example, see Fig. 7B, P2, head-restrained experiment, filled bars in Fig. 7A).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel B",
        "figure_key": "figure_7"
      },
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Using these individual points, we calculated the summary statistics shown in Fig. 7A and Table 3.",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Similar error distribution was observed for the head-unrestrained condition (open bars in Fig. 7A).",
    "figure_references": [
      {
        "figure_number": "Figure 7",
        "panel": "Panel A",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "Gaze point depth and vergence angle estimation error\nFigure 8A illustrates the binocular gaze point depth estimates in the 3D scene camera coordinate frame for all targets of the validation set while using the nine-point calibration choreography.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Figure 8B shows the average gaze point depth estimates for all calibration choreographies and testing sessions.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "On the whole, gaze point depth was severely underestimated for all marker positions and for all calibration choreographies used (with the exception of the five-point calibration) for P2, during the simultaneous EyeLink session (Fig. 8B).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "The offset between the scene camera and actual eye position leads to a parallax error (Gibaldi et al., 2021)While the median values and spread can provide a general idea of the gaze point depth error, as can be seen in Fig. 8A, the distribution of these errors depends on the visual angle distance from the center of the screen (Fig. 8C, Methods “Gaze point estimation” and “Binocular gaze point depth and vergence angle” sections).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Indeed, our observation was borne out: there was a significant negative correlation between marker eccentricity from the center and the depth of the gaze estimate for all calibrations (Spearman r = [− 0.64, − 0.63, – 0.62, − 0.69, − 0.67], p < 0.001, Fig. 8C shows example correlation for P2, head-unrestrained condition).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Fig. 8D shows the impact of poor gaze point depth estimation on measured vergence angle.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel D",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Moreover, there was high variability in gaze point depth among the validation marker presentations (see Fig. 8A for example calibration set) even though all markers were in the same plane, either 1 or 1.8 m away from the scene camera (marked as dashed gray lines in Fig. 8B and illustrated in panel A).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Figure 8A illustrates the binocular gaze point depth estimates in the 3D scene camera coordinate frame for all targets of the validation set while using the nine-point calibration choreography.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Figure 8B shows the average gaze point depth estimates for all calibration choreographies and testing sessions.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "On the whole, gaze point depth was severely underestimated for all marker positions and for all calibration choreographies used (with the exception of the five-point calibration) for P2, during the simultaneous EyeLink session (Fig. 8B).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "While the median values and spread can provide a general idea of the gaze point depth error, as can be seen in Fig. 8A, the distribution of these errors depends on the visual angle distance from the center of the screen (Fig. 8C, Methods “Gaze point estimation” and “Binocular gaze point depth and vergence angle” sections).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Indeed, our observation was borne out: there was a significant negative correlation between marker eccentricity from the center and the depth of the gaze estimate for all calibrations (Spearman r = [− 0.64, − 0.63, – 0.62, − 0.69, − 0.67], p < 0.001, Fig. 8C shows example correlation for P2, head-unrestrained condition).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Fig. 8D shows the impact of poor gaze point depth estimation on measured vergence angle.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel D",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Moreover, there was high variability in gaze point depth among the validation marker presentations (see Fig. 8A for example calibration set) even though all markers were in the same plane, either 1 or 1.8 m away from the scene camera (marked as dashed gray lines in Fig. 8B and illustrated in panel A).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel A",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Comparing with Fig. 8B, there is an inverse relationship between the gaze point depth and parallax error.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel B",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Summary statistics for parallax error for data shown in Fig. 9*\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head-restrainedMean ± std / median [q25 q75]\nP2 Head-unrestrained\nP1 with Eyelink\nP2 with Eyelink\n\n\n\n\n\n\n\n5-point\n0.076 [0.057 0.109]\n0.020 [0.013 0.068]\n\n0.022 ± 0.006\n\n\n0.008 ± 0.008\n\n\n\n9-point\n0.044 [0.028 0.070]\n0.029 [0.016 0.076]\n\n0.016 ± 0.006\n\n0.008 [0.004 0.014]\n\n\n13-point\n0.013 [0.009 0.044]\n0.042 [0.029 0.091]\n\n0.022 ± 0.006\n\n0.008 [0.003 0.011]\n\n\nStar\n0.130 [0.102 0.162]\n0.137 [0.103 0.189]\n\n0.009 ± 0.005\n\n0.020 [0.014 0.022]\n\n\naVOR\n\n0.022 [0.012 0.065]\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*For data distributed normally, mean, and standard deviation are included (bold text).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Summary statistics for parallax error for data shown in Fig. 9*.",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Summary statistics for parallax error for data shown in Fig. 9*.",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "The measurement error on screen was calculated in three ways (illustrated in Fig. 9A, see Methods 4.8.2) for both head-restrained (Fig. 9B) and head-unrestrained conditions (see Table 5 for comparison of all conditions/participants).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel A",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 9",
        "panel": "Panel B",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 9",
        "panel": "Panel B",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 9",
        "panel": "Panel B",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "First, the error is calculated as the distance between the onscreen intersection point of the cyclopean gaze ray and the respective fixation target locations of the validation set under each calibration (Fig. 9, orange).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Second, we repeated the calculation for the right and left eye gaze rays individually (Fig. 9, blue and red).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Finally, we estimated the distance between the screen intersection points of the right and left eye (Fig. 9, seafoam).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "To compare our errors to those measured by the EyeLink, we also performed a homography transformation between the camera image and the display screen in pixels and scaled to physical distances using the pixel per meter constant (see “Measured error in 2D: scene camera image and display screen“ section in Methods and “Error in 2D: scene camera image and display screen“ section in Results, Fig. 9C).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel C",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Errors calculated using this method are superimposed with the cyclopean error in orange (orange bars from Fig. 9B/Table 5, “Cyclopean” columns).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel B",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Importantly, the relative rotation angles for each eye are invariant to the reference coordinate frame, and are thus not affected by the choice of calibration (Fig. 1, column 1).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "As the calibration matrix just transforms the reference frame of the eye camera to the reference frame of the scene camera (Fig. 1, column 2), the eye rotation will be the same in both reference frames.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "For each recording session, we examined saccade amplitude for the rotation of the cyclopean gaze, the left, and the right eye (akin to the illustration in Fig. 9A, see “Measured error in 2D: scene camera image and display screen“ and “Saccade amplitude error estimation: Intermittent eye motion between two target locations“ sections in Methods).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel A",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "Thus, in Fig. 10 we show data for a single calibration (nine-point).",
    "figure_references": [
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      }
    ]
  },
  {
    "sentence": "There was significant variation in the median saccade size across the sessions (4° and 10°, Fig. 10A & B).",
    "figure_references": [
      {
        "figure_number": "Figure 10",
        "panel": "Panel A",
        "figure_key": "figure_10"
      }
    ]
  },
  {
    "sentence": "Overall, eye-in-head angular velocity exceeded head-in-space velocity by 15–17 %, for a participant viewing a distant (180 cm) target (Fig. 11E, F).",
    "figure_references": [
      {
        "figure_number": "Figure 11",
        "panel": "Panel E",
        "figure_key": "figure_11"
      }
    ]
  },
  {
    "sentence": "Overall, eye-in-head angular velocity exceeded head-in-space velocity by 15–17 %, for a participant viewing a distant (180 cm) target (Fig. 11E, F).",
    "figure_references": [
      {
        "figure_number": "Figure 11",
        "panel": "Panel E",
        "figure_key": "figure_11"
      }
    ]
  },
  {
    "sentence": "These experiments span several stages of the eye-tracking pipeline (Fig. 1) and experimenters should assess and mitigate those that are relevant to their specific experiment type.",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "First, motion of the eye cameras relative to each other and the rest of the device may not only introduce artificial movement of the eyes but invalidate the coordinate frame transformations computed during calibration (Fig. 4).",
    "figure_references": [
      {
        "figure_number": "Figure 4",
        "panel": "",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "",
        "figure_key": "figure_4"
      }
    ]
  },
  {
    "sentence": "Second, the choice of a calibration grid can affect eye orientation estimates (Fig. 5) that cannot be parsed without a ground truth reference.",
    "figure_references": [
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      }
    ]
  },
  {
    "sentence": "The headset was tightly secured to the head using an adjustable rigid plastic band (Fig. 3) to ensure that there was no movement between it and the participant.",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "The walk and run conditions yielded the least eye camera motion for both eyes (~ 12% of total movement), while jumping yielded over 50% more eye camera motion than walking, particularly in the right eye (Fig. 4E).",
    "figure_references": [
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      }
    ]
  },
  {
    "sentence": "While walking had the lowest vertical accelerations (Fig. 4F), running was associated with higher accelerations.",
    "figure_references": [
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      }
    ]
  },
  {
    "sentence": "The headset was tightly secured to the head using an adjustable rigid plastic band (Fig. 3) to ensure that there was no movement between it and the participant.",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "The walk and run conditions yielded the least eye camera motion for both eyes (~ 12% of total movement), while jumping yielded over 50% more eye camera motion than walking, particularly in the right eye (Fig. 4E).",
    "figure_references": [
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel E",
        "figure_key": "figure_4"
      }
    ]
  },
  {
    "sentence": "While walking had the lowest vertical accelerations (Fig. 4F), running was associated with higher accelerations.",
    "figure_references": [
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      },
      {
        "figure_number": "Figure 4",
        "panel": "Panel F",
        "figure_key": "figure_4"
      }
    ]
  },
  {
    "sentence": "Gaze estimation noise\nIn the second set of experiments, we evaluated Pupil Core’s performance in three different spatial domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the 3D space of the scene camera (not referenced to the external world, Fig. 1, column 3); and (3) relative to the physical marker location in 3D external world (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Further, we find that while the accuracy of the cyclopean gaze position may still appear relatively high, eye rotation estimation accuracy tends to be very poor and leads to viewing distance underestimation (Fig. 8).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "While this information can inform certain experiments, our data demonstrate that it does not provide sufficient information to help the experimenter choose an optimal calibration, predict the amount or variability of assessed parallax error, or reliably calculate eye rotation amplitudes (e.g., Fig. 10).",
    "figure_references": [
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      }
    ]
  },
  {
    "sentence": "For a single fixation target, the misestimation of the binocular gaze point away from the calibration plane results in incorrect vergence angle between the gaze rays of the two eyes, a positional shift between monocular points of regard on screen (and thus an increase in distance between the projections of the two eyes), and parallax error on the viewing screen (i.e., the calibration plane, see Fig. 8E for illustration).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel E",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "This variability in depth estimation of the binocular gaze point leads to a variable vergence angle estimates, large offsets of the eye position estimate relative to the marker (Fig. 8), and non-constant bias in parallax error (Table 4).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "In the second set of experiments, we evaluated Pupil Core’s performance in three different spatial domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the 3D space of the scene camera (not referenced to the external world, Fig. 1, column 3); and (3) relative to the physical marker location in 3D external world (Fig. 1, column 4).",
    "figure_references": [
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      },
      {
        "figure_number": "Figure 1",
        "panel": "",
        "figure_key": "figure_1"
      }
    ]
  },
  {
    "sentence": "Further, we find that while the accuracy of the cyclopean gaze position may still appear relatively high, eye rotation estimation accuracy tends to be very poor and leads to viewing distance underestimation (Fig. 8).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "While this information can inform certain experiments, our data demonstrate that it does not provide sufficient information to help the experimenter choose an optimal calibration, predict the amount or variability of assessed parallax error, or reliably calculate eye rotation amplitudes (e.g., Fig. 10).",
    "figure_references": [
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      }
    ]
  },
  {
    "sentence": "For a single fixation target, the misestimation of the binocular gaze point away from the calibration plane results in incorrect vergence angle between the gaze rays of the two eyes, a positional shift between monocular points of regard on screen (and thus an increase in distance between the projections of the two eyes), and parallax error on the viewing screen (i.e., the calibration plane, see Fig. 8E for illustration).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel E",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "This variability in depth estimation of the binocular gaze point leads to a variable vergence angle estimates, large offsets of the eye position estimate relative to the marker (Fig. 8), and non-constant bias in parallax error (Table 4).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "First, we found that depending on the calibration choreography, gaze orientation can vary significantly for a representative fixation estimate (Fig. 5).",
    "figure_references": [
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      }
    ]
  },
  {
    "sentence": "Further, the choice of calibration could have a significant effect on the estimation of the gaze point in depth, and therefore vergence angle (Fig. 8).",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "The compact nine-point star calibration had by far the most different outcomes in terms of camera position estimate (Fig. 5), variability in accuracy across the locations of the validation grid (Fig. 7), and position estimation error for each eye (and the distance between them, Fig. 9).",
    "figure_references": [
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 7",
        "panel": "",
        "figure_key": "figure_7"
      },
      {
        "figure_number": "Figure 9",
        "panel": "",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 7",
        "panel": "",
        "figure_key": "figure_7"
      },
      {
        "figure_number": "Figure 5",
        "panel": "",
        "figure_key": "figure_5"
      },
      {
        "figure_number": "Figure 7",
        "panel": "",
        "figure_key": "figure_7"
      }
    ]
  },
  {
    "sentence": "This latter may be related to the significant gaze depth underestimates and resultant vergence angle overestimates (Fig. 8) for the star calibration.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Notably, although the compact star may seem like the least desirable calibration based on these metrics, it was also one that had the least variable gaze depth estimates (Fig. 8C) and may thus be easiest to correct using a spatial transformation.",
    "figure_references": [
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      },
      {
        "figure_number": "Figure 8",
        "panel": "Panel C",
        "figure_key": "figure_8"
      }
    ]
  },
  {
    "sentence": "Generally, the use of a homography transformation, when possible, may improve eye position accuracy (Fig. 9C).",
    "figure_references": [
      {
        "figure_number": "Figure 9",
        "panel": "Panel C",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 9",
        "panel": "Panel C",
        "figure_key": "figure_9"
      },
      {
        "figure_number": "Figure 9",
        "panel": "Panel C",
        "figure_key": "figure_9"
      }
    ]
  },
  {
    "sentence": "However, the mask to which the printed eyes were attached was rigid plastic (thus not allowing for any slippage due to skin deformation) and the headset was secured very tightly to the head using a ratcheting restraint (Fig. 3).",
    "figure_references": [
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      },
      {
        "figure_number": "Figure 3",
        "panel": "",
        "figure_key": "figure_3"
      }
    ]
  },
  {
    "sentence": "We observed similar amplitude variability between the two devices for both participants, though the mean amplitudes did tend to differ between the two (Fig. 10).",
    "figure_references": [
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      },
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      },
      {
        "figure_number": "Figure 10",
        "panel": "",
        "figure_key": "figure_10"
      }
    ]
  }
]