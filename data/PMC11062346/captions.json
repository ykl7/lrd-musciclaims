{
  "figure_1": "Computational domains of the Pupil Core eye-tracking system. Column 1 represents the least computationally intensive data stream available from the device. Locations of the pupil center are provided in pixel coordinates of the eye camera images. This data stream can provide relative information regarding pupil movement or size. Importantly, movement of the pupil relative to the camera or camera relative to the pupil cannot be distinguished. Column 2 represents the next step in processing of the eye movement signal. The pupil position in the eye camera image is transformed into the reference frame of the scene camera using a calibration procedure. This step can provide eye position relative to objects in the 2D scene camera image. Column 3 represents eye rotations in the scene camera reference frame computed relative to the three-dimensional estimate of the scene seen by the world camera. Additionally, orientations of each eye can be compared with the other. A validation procedure can be used to estimate error of the eye orientation estimates from a given eye model. Column 4 represents the most computationally intensive step, requiring additional hardware to transform the scene camera coordinate frame into the world coordinates. This step is required whenever eye movements are measured relative to an external reference, such as head or body movement in space",
  "figure_2": "Experimental design for studies 1 and 2. A Study 1 examines signal noise due to the movement of the eye cameras relative to the participant and the scene camera. Camera motion was examined while tracking a printout of the eyes (eyes are fixed, all motion due to camera) and compared to human eye movement in four motion conditions: walk, run, march, and jump. B Study 2 examines signal noise due to successive computational steps required to estimate eye movements in the different reference frames of Fig. 1. Participants were tested with the head either restrained or unrestrained, two of the head restrained sessions were done using the EyeLink 1000 as well as the Pupil Core. All computations were done using one of four grid calibrations (C) or aVOR (only possible when head was unrestrained) and tested on a 25-point validation grid. Saccadic and aVOR eye movements were also examined. aVOR graph illustrates central marker’s position in the scene camera image during the VOR (vestibuloocular reflex) task. Saccade schematics illustrate the location of the visual stimulus when the saccade task was performed. C Sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers placed in different grid patterns and the set of 25 fixation marker locations used for validation (all calibration marker locations combined). Placement of these points in the different reference frames (camera sensor, screen, eye rotation angle) is shown in Fig. S2",
  "figure_3": "Camera mobility during four tasks: walking, marching, running, and jumping. A Ratio of fxed vs. human eye motion in the scene camera of the right (circle) and left (square) fxed vs. human eye along the horizontal (filed symbols) and vertical (open symbols) axes. B The peak-to-peak amplitude of the linear acceleration of the head along the gravitational direction for all four tasks during human (filled circles) and fxed eyes mask (open circles) experiments. Yellow: P1, orange: P2, maroon: P3, purple: P4",
  "figure_4": "A Schematic representation of the diferent error metrics (Cyclopean, Left, Right, Left-Right Distance) calculated on screen of the P2, head restrained (B) and P2, head-unrestrained, P1 with EyeLink and P2 with EyeLink conditions (Table 5). C Measurement errors for all participants, head unrestrained conditions, calculated using a homography transformation of the two-dimensional gaze points onto the display screen (gray) and the cyclopean errors (orange) from Table 5. Error calculated from the EyeLink data is shown in green for sessions where available",
  "figure_5": "Measured error in the 2D scene camera image and on the display screen: A Example set of reference points and gaze point locations in the 2D scene camera image for P2, using the nine-point calibration grid and evaluated over the validation set. The red lines represent the measured error in pixels and correspond to the last yellow box in panel C. B Gaze points estimated by the Pupil Core algorithm (red Xs) from A, mapped onto the display screen using a 2D projective homography, target locations (black and gray) and the respective EyeLink gaze points (blue Xs). The lines, red for PupilCore and blue for the EyeLink, represent the measured error in pixels on the display screen. The summary statistics for this specific distribution converted to degrees are shown in the last yellow box in panel D (nine-point calibration, P2 with EyeLink). C Measured error in pixels in the 2D scene camera for all four data sets: P2-head restrained (full boxes), head unrestrained (empty boxes), P1 head restrained, EyeLink (hashed left), P2 head restrained, EyeLink (hashed right), and all calibration sets: 5 (blue), 9 (yellow), 13 points (purple), star (red) and aVOR (black). D Measured error in degrees, for the data recorded with PupilCore in all head restrained experiments. Error for the EyeLink data is shown in green for comparison",
  "figure_6": "Saccades amplitude for the small (4°, A) and large (10°, B) saccade tasks for each eye (all sessions) using the nine-point calibration. Dashed lines – ideal, cyclopean gaze (orange), right (red) and left (navy) rotation amplitudes are shown, along with saccade amplitudes calculated using a projection onto the display screen (gray) and those measured using the EyeLink 1000 (green). Gaze point depth estimates for the rightward saccade locations, for large (B) saccades using diferent calibrations. Dashed lines – actual fxation depth",
  "figure_7": "Eye camera orientations in screen camera coordinates following four diferent calibrations: 5 points (blue), 9 points (yellow), 13 points (lilac), and nine-point star (red). Dots of corresponding color demonstrate estimated eye camera position using that calibration. Inset shows eye camera coordinate frames estimated using each calibration demarcated with corresponding color lines at the positions shown. Black dots: estimated eye position, with estimated eye rotation (gaze vector) shown in corresponding colors to the calibration used for respective gaze ray estimation. θ is an example diference between eye rotations estimated with two diferent calibrations (Star & 13-point grids)",
  "figure_8": "Measurement accuracy error in the scene camera 3D space. A Median accuracy estimated across the 25 validation marker locations for each calibration (five-point: blue, nine-point: yellow, 13-point: purple, star: red, and aVOR: black) and head-restrained (filled bars) and head-unrestrained (open bars) conditions. Data for simultaneous EyeLink sessions are shown as bars with hashes to the left (P1) and right (P2). B Accuracy error shown for each of the 25 validation points and calibrations, for the P2, head-restrained experiment. Each dot represents error magnitude (vertical axis) computed with the corresponding calibration (see color legend) for a corresponding marker ID (shown at locations indicated in the inset)",
  "figure_9": "Measurement error in 3D gaze estimation. A Example spatial distribution of the binocular gaze points for the nine-point calibration (P2, head-restrained, no EyeLink condition) bullseye dots on the gray rectangle (screen) represent validation marker locations. Solid dots are estimated gaze point positions relative to the screen in the 3D space of the scene camera. The central marker was shown multiple times (brown dots). B Binocular gaze point (GZP) depth from the origin of the scene camera 3D coordinate frame. Horizontal dashed lines: actual screen depth. C Distribution of gaze depth estimates across target eccentricity from the screen center computed on the 25-point validation set for all five calibration choreographies (P2, head-unrestrained). Inset shows marker distances from center in degrees. D Vergence angles estimated for each validation location using each calibration. Horizontal dashed lines: ideal vergence angles (φ). All bar plots: filled boxes – P2, head-restrained; open boxes – P2, head-restrained; hashed to the right – P1, hashed to the left – P2 (last two in the EyeLink condition). All panels show errors using the validation set for each calibration choreography (blue: five-point, yellow: nine-point, purple: 13-point, red: star, black: aVOR). E Schematic illustration of vergence angle (D) and parallax error (Table 4) estimates. Due to an underestimated gaze depth, the gaze rays for each eye appear to intersect at a smaller depth and thus have a greater angle between them and are diverging at the actual screen depth. The offset between the scene camera and actual eye position leads to a parallax error (Gibaldi et al., 2021)",
  "figure_10": "VOR: Rotational angle (A, B) and angular velocity (C, D) in yaw (A, C) and pitch (B, D) for right (red dashed) and left (blue dashed) eye in head, head (black solid) in space and right (red solid) and left (blue solid) gaze in space when 13-point calibration was used to calculate the gaze. The gain was calculated as linear regression of angular velocity between head and each eye (right – red; left – blue) for the yaw (E) and pitch (F) motion, respectively"
}