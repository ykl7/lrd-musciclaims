{
  "paper_id": "PMC11062346",
  "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11062346/",
  "figures": {
    "figure_1": {
      "figure_number": "Figure 1",
      "title": "Fig. 1.",
      "caption": "Computational domains of the Pupil Core eye-tracking system. Column 1 represents the least computationally intensive data stream available from the device. Locations of the pupil center are provided in pixel coordinates of the eye camera images. This data stream can provide relative information regarding pupil movement or size. Importantly, movement of the pupil relative to the camera or camera relative to the pupil cannot be distinguished. Column 2 represents the next step in processing of the eye movement signal. The pupil position in the eye camera image is transformed into the reference frame of the scene camera using a calibration procedure. This step can provide eye position relative to objects in the 2D scene camera image. Column 3 represents eye rotations in the scene camera reference frame computed relative to the three-dimensional estimate of the scene seen by the world camera. Additionally, orientations of each eye can be compared with the other. A validation procedure can be used to estimate error of the eye orientation estimates from a given eye model. Column 4 represents the most computationally intensive step, requiring additional hardware to transform the scene camera coordinate frame into the world coordinates. This step is required whenever eye movements are measured relative to an external reference, such as head or body movement in space",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F1",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_1.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4a4211562e6a/nihms-1985850-f0001.jpg"
    },
    "figure_2": {
      "figure_number": "Figure 2",
      "title": "Fig. 2.",
      "caption": "Experimental design for studies 1 and 2. A Study 1 examines signal noise due to the movement of the eye cameras relative to the participant and the scene camera. Camera motion was examined while tracking a printout of the eyes (eyes are fixed, all motion due to camera) and compared to human eye movement in four motion conditions: walk, run, march, and jump. B Study 2 examines signal noise due to successive computational steps required to estimate eye movements in the different reference frames of Fig. 1. Participants were tested with the head either restrained or unrestrained, two of the head restrained sessions were done using the EyeLink 1000 as well as the Pupil Core. All computations were done using one of four grid calibrations (C) or aVOR (only possible when head was unrestrained) and tested on a 25-point validation grid. Saccadic and aVOR eye movements were also examined. aVOR graph illustrates central marker’s position in the scene camera image during the VOR (vestibuloocular reflex) task. Saccade schematics illustrate the location of the visual stimulus when the saccade task was performed. C Sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers placed in different grid patterns and the set of 25 fixation marker locations used for validation (all calibration marker locations combined). Placement of these points in the different reference frames (camera sensor, screen, eye rotation angle) is shown in Fig. S2",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F2",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_2.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/57350b3779f3/nihms-1985850-f0002.jpg"
    },
    "figure_3": {
      "figure_number": "Figure 3",
      "title": "Fig. 4.",
      "caption": "Camera mobility during four tasks: walking, marching, running, and jumping. A Ratio of fxed vs. human eye motion in the scene camera of the right (circle) and left (square) fxed vs. human eye along the horizontal (filed symbols) and vertical (open symbols) axes. B The peak-to-peak amplitude of the linear acceleration of the head along the gravitational direction for all four tasks during human (filled circles) and fxed eyes mask (open circles) experiments. Yellow: P1, orange: P2, maroon: P3, purple: P4",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F4",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_3.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c26e0917e441/nihms-1985850-f0004.jpg"
    },
    "figure_4": {
      "figure_number": "Figure 4",
      "title": "Fig. 9.",
      "caption": "A Schematic representation of the diferent error metrics (Cyclopean, Left, Right, Left-Right Distance) calculated on screen of the P2, head restrained (B) and P2, head-unrestrained, P1 with EyeLink and P2 with EyeLink conditions (Table 5). C Measurement errors for all participants, head unrestrained conditions, calculated using a homography transformation of the two-dimensional gaze points onto the display screen (gray) and the cyclopean errors (orange) from Table 5. Error calculated from the EyeLink data is shown in green for sessions where available",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F9",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_4.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/24e4336b5cf8/nihms-1985850-f0009.jpg"
    },
    "figure_5": {
      "figure_number": "Figure 5",
      "title": "Fig. 6.",
      "caption": "Measured error in the 2D scene camera image and on the display screen: A Example set of reference points and gaze point locations in the 2D scene camera image for P2, using the nine-point calibration grid and evaluated over the validation set. The red lines represent the measured error in pixels and correspond to the last yellow box in panel C. B Gaze points estimated by the Pupil Core algorithm (red Xs) from A, mapped onto the display screen using a 2D projective homography, target locations (black and gray) and the respective EyeLink gaze points (blue Xs). The lines, red for PupilCore and blue for the EyeLink, represent the measured error in pixels on the display screen. The summary statistics for this specific distribution converted to degrees are shown in the last yellow box in panel D (nine-point calibration, P2 with EyeLink). C Measured error in pixels in the 2D scene camera for all four data sets: P2-head restrained (full boxes), head unrestrained (empty boxes), P1 head restrained, EyeLink (hashed left), P2 head restrained, EyeLink (hashed right), and all calibration sets: 5 (blue), 9 (yellow), 13 points (purple), star (red) and aVOR (black). D Measured error in degrees, for the data recorded with PupilCore in all head restrained experiments. Error for the EyeLink data is shown in green for comparison",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F6",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_5.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/4e80378c3dd0/nihms-1985850-f0006.jpg"
    },
    "figure_6": {
      "figure_number": "Figure 6",
      "title": "Fig. 10.",
      "caption": "Saccades amplitude for the small (4°, A) and large (10°, B) saccade tasks for each eye (all sessions) using the nine-point calibration. Dashed lines – ideal, cyclopean gaze (orange), right (red) and left (navy) rotation amplitudes are shown, along with saccade amplitudes calculated using a projection onto the display screen (gray) and those measured using the EyeLink 1000 (green). Gaze point depth estimates for the rightward saccade locations, for large (B) saccades using diferent calibrations. Dashed lines – actual fxation depth",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F10",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_6.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/2dd4e7143e33/nihms-1985850-f0010.jpg"
    },
    "figure_7": {
      "figure_number": "Figure 7",
      "title": "Fig. 5.",
      "caption": "Eye camera orientations in screen camera coordinates following four diferent calibrations: 5 points (blue), 9 points (yellow), 13 points (lilac), and nine-point star (red). Dots of corresponding color demonstrate estimated eye camera position using that calibration. Inset shows eye camera coordinate frames estimated using each calibration demarcated with corresponding color lines at the positions shown. Black dots: estimated eye position, with estimated eye rotation (gaze vector) shown in corresponding colors to the calibration used for respective gaze ray estimation. θ is an example diference between eye rotations estimated with two diferent calibrations (Star & 13-point grids)",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F5",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_7.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/361bc4702873/nihms-1985850-f0005.jpg"
    },
    "figure_8": {
      "figure_number": "Figure 8",
      "title": "Fig. 7.",
      "caption": "Measurement accuracy error in the scene camera 3D space. A Median accuracy estimated across the 25 validation marker locations for each calibration (five-point: blue, nine-point: yellow, 13-point: purple, star: red, and aVOR: black) and head-restrained (filled bars) and head-unrestrained (open bars) conditions. Data for simultaneous EyeLink sessions are shown as bars with hashes to the left (P1) and right (P2). B Accuracy error shown for each of the 25 validation points and calibrations, for the P2, head-restrained experiment. Each dot represents error magnitude (vertical axis) computed with the corresponding calibration (see color legend) for a corresponding marker ID (shown at locations indicated in the inset)",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F7",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_8.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/302708b65588/nihms-1985850-f0007.jpg"
    },
    "figure_9": {
      "figure_number": "Figure 9",
      "title": "Fig. 8.",
      "caption": "Measurement error in 3D gaze estimation. A Example spatial distribution of the binocular gaze points for the nine-point calibration (P2, head-restrained, no EyeLink condition) bullseye dots on the gray rectangle (screen) represent validation marker locations. Solid dots are estimated gaze point positions relative to the screen in the 3D space of the scene camera. The central marker was shown multiple times (brown dots). B Binocular gaze point (GZP) depth from the origin of the scene camera 3D coordinate frame. Horizontal dashed lines: actual screen depth. C Distribution of gaze depth estimates across target eccentricity from the screen center computed on the 25-point validation set for all five calibration choreographies (P2, head-unrestrained). Inset shows marker distances from center in degrees. D Vergence angles estimated for each validation location using each calibration. Horizontal dashed lines: ideal vergence angles (φ). All bar plots: filled boxes – P2, head-restrained; open boxes – P2, head-restrained; hashed to the right – P1, hashed to the left – P2 (last two in the EyeLink condition). All panels show errors using the validation set for each calibration choreography (blue: five-point, yellow: nine-point, purple: 13-point, red: star, black: aVOR). E Schematic illustration of vergence angle (D) and parallax error (Table 4) estimates. Due to an underestimated gaze depth, the gaze rays for each eye appear to intersect at a smaller depth and thus have a greater angle between them and are diverging at the actual screen depth. The offset between the scene camera and actual eye position leads to a parallax error (Gibaldi et al., 2021)",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F8",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_9.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/c0e712e60870/nihms-1985850-f0008.jpg"
    },
    "figure_10": {
      "figure_number": "Figure 10",
      "title": "Fig. 11.",
      "caption": "VOR: Rotational angle (A, B) and angular velocity (C, D) in yaw (A, C) and pitch (B, D) for right (red dashed) and left (blue dashed) eye in head, head (black solid) in space and right (red solid) and left (blue solid) gaze in space when 13-point calibration was used to calculate the gaze. The gain was calculated as linear regression of angular velocity between head and each eye (right – red; left – blue) for the yaw (E) and pitch (F) motion, respectively",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F11",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11062346/images/figure_10.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4cef/11062346/d7d4acbac876/nihms-1985850-f0011.jpg"
    }
  },
  "claims": [
    {
      "sentence": "There are several applications where the two-dimensional gaze position mapping is useful (Fig. 1, column 2).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "To that end, as designed, the algorithm provides the researcher with a 3D eye model necessary for the estimation of rotational eye angles relative to the volume in the scene camera image that can be used to calculate eye movements relative to objects in this volume and each other, such as vergence angles (Fig. 1, column 3).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "With the addition of an external reference sensor (such as an inertial measurement unit) these data can be further processed to assess eye movements relative to head movement (such as the angular vestibuloocular reflex, aVOR) and to the external world, such as saccadic or smooth gaze shifts (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "While there are experiments that rely on all iterations of the eye movement data stream, each data stream relies on a particular set of assumptions and requires additional processing steps (Fig. 1) that may add measurement uncertainty that is not assessed in the standard calibration accuracy testing protocols.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "To that end, the goal of this paper is to provide experimenters with a roadmap of the potential pitfalls that may affect eye movement data along the processing pipeline illustrated in Fig. 1, and how these can be ascertained for a given experimental setup/protocol.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "In this work, we focus on the sources of noise in the following domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the physical rotation of the eye in scene camera 3D space – a representation essential for understanding eye movements relative to the objects in the scene and each other (Fig. 1, column 3); and (3) the external projection of the estimated gaze point location onto the target plane or in relation to world coordinates (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Currently, to understand the error in each recording, the Pupil Capture software provides the researcher with qualitative visual feedback of eye position accuracy in the scene camera image (Fig. 1, column 2).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "An average accuracy estimate of eye rotation relative to the target is also available, but only if additional validation is performed (Fig. 1, column 3).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "These latter experiments address the assumption of tracker stability relative to the head during pupil detection and eye model estimation in the eye camera images (Fig. 1, column 1).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "However, eye model estimation (Fig. 1, column 2) relies not only on accurate pupil detection but also requires that there is no camera movement between the three (two eye and one scene) cameras.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Thus, in Study 1, we investigate the potential for movement between the three cameras as a source of error that may be introduced early in the eye-tracking pipeline (Study 1 in Fig. 2A).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "We find that depending on the type of natural behavior (Fig. 2B), substantial eye camera motion is possible independent of the scene camera and may need to be considered in experiments depending on the degree of head/body movement required.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        },
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        },
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "In Study 2, we investigate sources of computational noise that can affect the two- and three-dimensional eye position and orientation estimates (Study 2 in Fig. 2A).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "First, we identify potential errors in gaze estimation in the 2D scene camera and how they might be affected by different calibration routines (Fig. 2C).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        },
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        },
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Experimental protocol: Eye cameras mobility evaluation\nTo evaluate the effect of eye camera mobility, two sets of experiments were performed with four tasks each (see Fig. 2, Study 1).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "In one set of experiments, the eye tracker recorded the human participants’ pupils, while in the static eye experiment, the eye tracker was recording printed images of eyes placed on a mask worn by the participants (shown in Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "To evaluate the effect of eye camera mobility, two sets of experiments were performed with four tasks each (see Fig. 2, Study 1).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "In one set of experiments, the eye tracker recorded the human participants’ pupils, while in the static eye experiment, the eye tracker was recording printed images of eyes placed on a mask worn by the participants (shown in Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Equipment\nParticipants were equipped with the Pupil Core headset secured via adjustable, ratcheting headgear (Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "All cameras are placed on a lightweight, head-mounted frame (22.75 g, Pupil Labs, 2022) and can be moved to adjust for better capture of the pupils or the scene of interest (Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "A wired inertial measurement unit (IMU, LPMS-CURS2, LPMS Research, Tokyo, Japan) with 9 degrees of freedom (three accelerometer, three gyroscope, and three magnetometer axes) was firmly attached to the eye tracker frame using a custom mount (total IMU + enclosure weight = 14.17 g, Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Participants were equipped with the Pupil Core headset secured via adjustable, ratcheting headgear (Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "All cameras are placed on a lightweight, head-mounted frame (22.75 g, Pupil Labs, 2022) and can be moved to adjust for better capture of the pupils or the scene of interest (Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "A wired inertial measurement unit (IMU, LPMS-CURS2, LPMS Research, Tokyo, Japan) with 9 degrees of freedom (three accelerometer, three gyroscope, and three magnetometer axes) was firmly attached to the eye tracker frame using a custom mount (total IMU + enclosure weight = 14.17 g, Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Four tasks were performed (see Fig. 2, Study 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The participant was asked to fixate a succession of four sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers shown sequentially in different grid patterns (Fig. 2C, Calibration).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Second, a grid fixation task with 25 marker positions (Fig. 2C, Validation) was performed in a similar manner and used for validation (test set).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "In other words, the participant was asked to make 10° amplitude saccades (third task) and 4° amplitude saccades (fourth task), respectively (Fig. 2A, Saccades).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The experiment was also performed with the head unrestrained (Fig. 2A, Head Unrestrained), and the participant having been asked to keep it as still as possible during the grid fixation and saccade tasks.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The central marker’s frame-by-frame position in the scene camera image during the aVOR task is presented in (Fig. 2A, aVOR).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Four tasks were performed (see Fig. 2, Study 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The participant was asked to fixate a succession of four sets of visual stimuli formed by 5, 9, 13, and 9 fixed markers shown sequentially in different grid patterns (Fig. 2C, Calibration).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Second, a grid fixation task with 25 marker positions (Fig. 2C, Validation) was performed in a similar manner and used for validation (test set).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "In other words, the participant was asked to make 10° amplitude saccades (third task) and 4° amplitude saccades (fourth task), respectively (Fig. 2A, Saccades).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The experiment was also performed with the head unrestrained (Fig. 2A, Head Unrestrained), and the participant having been asked to keep it as still as possible during the grid fixation and saccade tasks.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "The central marker’s frame-by-frame position in the scene camera image during the aVOR task is presented in (Fig. 2A, aVOR).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "An IMU was attached to the headset as in Study 1 and aligned with the scene camera such that the coordinate frames of both sensors were parallel and the axes aligned in the same direction [Fig. 3, also see (Velisar & Shanidze, 2021)].",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "An IMU was attached to the headset as in Study 1 and aligned with the scene camera such that the coordinate frames of both sensors were parallel and the axes aligned in the same direction [Fig. 3, also see (Velisar & Shanidze, 2021)].",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "The measurement error produced by the five calibration choreographies evaluated in this study was measured in three different spaces: (1) the 2D scene camera image in which a projection of the gaze point was compared with the respective reference point (Fig. 1, column 2); (2) the eye rotation angles between successive fixation positions, the vergence angles and the gaze point depth estimates in the 3D scene camera space (Fig. 1, column 3); and (3) the eye positions on the screen and eye rotations relative to the head in the world-centered coordinate frame (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The Euclidian distance between the gaze point and target was calculated to estimate the measurement error in meters on the screen (see Fig. 9 in Results for reference) for the validation set and the saccade amplitude, the Euclidian distance between successive saccades starting positions.",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "The Euclidian distance between the gaze point and target was calculated to estimate the measurement error in meters on the screen (see Fig. 9 in Results for reference) for the validation set and the saccade amplitude, the Euclidian distance between successive saccades starting positions.",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "We used this process to calculate each calibration’s accuracy and precision over the validation set (Fig. 2C, Validation).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head Restrained\n\nP2 Head Unrestrained\n\nP1 with EyeLink\n\nP2 with EyeLink\n\n\n\n\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\n\n\n\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\n\n\n\n\n\n\n\n5 pts\n2.0 [1.4 2.4]\n\n0.9\n\n\n2.5 ± 1.1\n\n\n0.5\n\n1.5 [1.0 1.8]\n\n0.3\n\n1.4 [0.8 2.6]\n\n0.2\n\n\n\n9 pts\n\n1.8 ± 1.0\n\n\n0.8\n\n\n2.6 ± 1.0\n\n\n0.4\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.9 [1.2 2.5]\n\n0.2\n\n\n\n13 pts\n\n1.8 ± 0.9\n\n\n0.8\n\n1.9 [1.4 2.8]\n\n0.5\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.5 [1.1 2.7]\n\n0.2\n\n\n\nStar\n1.5 [1.1 2.5]\n\n0.4\n\n2.6 [1.1 4.7]\n\n0.5\n\n1.0 [0.5 2.1]\n\n0.1\n\n1.6 [1.1 3.3]\n\n0.4\n\n\n\naVOR\n\n\n\n2.9 ± 0.9\n\n\n0.4\n\n\n\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*All accuracy calculations were done for the 25 validations points.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "We used this process to calculate each calibration’s accuracy and precision over the validation set (Fig. 2C, Validation).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head Restrained\n\nP2 Head Unrestrained\n\nP1 with EyeLink\n\nP2 with EyeLink\n\n\n\n\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\nAccuracy [°]\nPrecision [°]\n\n\n\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\nMean ± std median [q25 q75]\nstd\n\n\n\n\n\n\n\n5 pts\n2.0 [1.4 2.4]\n\n0.9\n\n\n2.5 ± 1.1\n\n\n0.5\n\n1.5 [1.0 1.8]\n\n0.3\n\n1.4 [0.8 2.6]\n\n0.2\n\n\n\n9 pts\n\n1.8 ± 1.0\n\n\n0.8\n\n\n2.6 ± 1.0\n\n\n0.4\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.9 [1.2 2.5]\n\n0.2\n\n\n\n13 pts\n\n1.8 ± 0.9\n\n\n0.8\n\n1.9 [1.4 2.8]\n\n0.5\n\n\n1.2 ± 0.8\n\n\n0.2\n\n1.5 [1.1 2.7]\n\n0.2\n\n\n\nStar\n1.5 [1.1 2.5]\n\n0.4\n\n2.6 [1.1 4.7]\n\n0.5\n\n1.0 [0.5 2.1]\n\n0.1\n\n1.6 [1.1 3.3]\n\n0.4\n\n\n\naVOR\n\n\n\n2.9 ± 0.9\n\n\n0.4\n\n\n\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*All accuracy calculations were done for the 25 validations points.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Summary statistics for gaze rotation angle accuracy and precision in Fig. 7A*.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel A",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel A",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "In addition, the distance between the two monocular gaze ray intersection points with the screen was calculated and reported in the results as L-R distance (see Fig. 9A in Results for schematic and “On screen, in the world coordinate frame“ section for Results).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel A",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "To determine whether differences in the arrangement of calibration targets affect gaze estimation, we performed four head-restrained calibration choreographies: 5, 9, and 13 point full-screen grids and a nine-point central cross (star, Fig. 2C) and five head-unrestrained choreographies: the four used in the head-restrained calibrations and a single marker head movement calibration (aVOR, Fig. 2A, C).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel C",
          "figure_key": "figure_2"
        },
        {
          "figure_number": "Figure 2",
          "panel": "Panel A",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Furthermore, it is not clear if the same calibration approach would be the best choice for experiments in each of the reference frames described in Fig. 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "However, these have largely been reported only relative to the scene camera image (Fig. 1, column 3) and for the combined gaze point across the two eyes.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Further, we examine how the choice of calibration might affect these estimates in each analysis domain: (1) pixel distance in the scene camera image (Fig. 1, column 2), (2) as the angular difference between the binocular gaze point and the unprojected target locations in the scene camera space (Fig. 1, column 3), and (3) distance (meters) on the physical screen (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "The largest variability was observed for the head-unrestrained data, calibrated using the star calibration (Fig. 6C, red unfilled box).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel C",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Figure 6A shows the reference points and an example set of gaze point locations in the 2D scene camera image for gaze calibrated with the nine-point calibration choreography and evaluated over the validation set (P2, simultaneous EyeLink session).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel A",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Figure 6B shows the data set in Fig. 6A mapped on the display screen (red Xs).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel B",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "Panel A",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "By using the same pixel per degree conversion used by the EyeLink, the data can be represented in degrees of visual angle (shown in Fig. 6D).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel D",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "As can be observed in Fig. 6D, the errors measured by Pupil Core are larger than those from the EyeLink’s for all data sets and all calibrations.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel D",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "The averaged errors shown here are comparable with the values reported by the Pupil Core software (Table 2) and the average errors calculated using the 3D eye rotations (shown in Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "The largest variability was observed for the head-unrestrained data, calibrated using the star calibration (Fig. 6C, red unfilled box).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel C",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Figure 6A shows the reference points and an example set of gaze point locations in the 2D scene camera image for gaze calibrated with the nine-point calibration choreography and evaluated over the validation set (P2, simultaneous EyeLink session).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel A",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Figure 6B shows the data set in Fig. 6A mapped on the display screen (red Xs).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel B",
          "figure_key": "figure_6"
        },
        {
          "figure_number": "Figure 6",
          "panel": "Panel A",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "By using the same pixel per degree conversion used by the EyeLink, the data can be represented in degrees of visual angle (shown in Fig. 6D).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel D",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "As can be observed in Fig. 6D, the errors measured by Pupil Core are larger than those from the EyeLink’s for all data sets and all calibrations.",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "Panel D",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "The averaged errors shown here are comparable with the values reported by the Pupil Core software (Table 2) and the average errors calculated using the 3D eye rotations (shown in Fig. 7).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Pupil capture accuracy visualizer\nWe report the accuracy and precision estimates (Fig. 1, column 3) given by the Pupil software (Pupil Labs’ accuracy visualizer plug-in) for gaze points calculated for the validation set using each calibration (see Methods “Measured error in 2D: scene camera image and display screen“ section).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "We report the accuracy and precision estimates (Fig. 1, column 3) given by the Pupil software (Pupil Labs’ accuracy visualizer plug-in) for gaze points calculated for the validation set using each calibration (see Methods “Measured error in 2D: scene camera image and display screen“ section).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Gaze estimation accuracy in the 3D camera space\nTo calculate the accuracy (or more precisely the angle error, between the gaze point position vector and the respective unprojected reference point in the scene camera coordinate frame, Fig. 1, column 3) for all validation target fixations we implemented a similar algorithm to that above.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Unlike Pupil Core’s own single value estimate, this approach allowed us to estimate error at each spatial location of targets on screen, for each calibration (for example, see Fig. 7B, P2, head-restrained experiment, filled bars in Fig. 7A).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel B",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Using these individual points, we calculated the summary statistics shown in Fig. 7A and Table 3.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Similar error distribution was observed for the head-unrestrained condition (open bars in Fig. 7A).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "To calculate the accuracy (or more precisely the angle error, between the gaze point position vector and the respective unprojected reference point in the scene camera coordinate frame, Fig. 1, column 3) for all validation target fixations we implemented a similar algorithm to that above.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Unlike Pupil Core’s own single value estimate, this approach allowed us to estimate error at each spatial location of targets on screen, for each calibration (for example, see Fig. 7B, P2, head-restrained experiment, filled bars in Fig. 7A).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel B",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Using these individual points, we calculated the summary statistics shown in Fig. 7A and Table 3.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Similar error distribution was observed for the head-unrestrained condition (open bars in Fig. 7A).",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "Panel A",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Gaze point depth and vergence angle estimation error\nFigure 8A illustrates the binocular gaze point depth estimates in the 3D scene camera coordinate frame for all targets of the validation set while using the nine-point calibration choreography.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Figure 8B shows the average gaze point depth estimates for all calibration choreographies and testing sessions.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "On the whole, gaze point depth was severely underestimated for all marker positions and for all calibration choreographies used (with the exception of the five-point calibration) for P2, during the simultaneous EyeLink session (Fig. 8B).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "The offset between the scene camera and actual eye position leads to a parallax error (Gibaldi et al., 2021)While the median values and spread can provide a general idea of the gaze point depth error, as can be seen in Fig. 8A, the distribution of these errors depends on the visual angle distance from the center of the screen (Fig. 8C, Methods “Gaze point estimation” and “Binocular gaze point depth and vergence angle” sections).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Indeed, our observation was borne out: there was a significant negative correlation between marker eccentricity from the center and the depth of the gaze estimate for all calibrations (Spearman r = [− 0.64, − 0.63, – 0.62, − 0.69, − 0.67], p < 0.001, Fig. 8C shows example correlation for P2, head-unrestrained condition).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Fig. 8D shows the impact of poor gaze point depth estimation on measured vergence angle.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel D",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Moreover, there was high variability in gaze point depth among the validation marker presentations (see Fig. 8A for example calibration set) even though all markers were in the same plane, either 1 or 1.8 m away from the scene camera (marked as dashed gray lines in Fig. 8B and illustrated in panel A).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Figure 8A illustrates the binocular gaze point depth estimates in the 3D scene camera coordinate frame for all targets of the validation set while using the nine-point calibration choreography.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Figure 8B shows the average gaze point depth estimates for all calibration choreographies and testing sessions.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "On the whole, gaze point depth was severely underestimated for all marker positions and for all calibration choreographies used (with the exception of the five-point calibration) for P2, during the simultaneous EyeLink session (Fig. 8B).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "While the median values and spread can provide a general idea of the gaze point depth error, as can be seen in Fig. 8A, the distribution of these errors depends on the visual angle distance from the center of the screen (Fig. 8C, Methods “Gaze point estimation” and “Binocular gaze point depth and vergence angle” sections).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Indeed, our observation was borne out: there was a significant negative correlation between marker eccentricity from the center and the depth of the gaze estimate for all calibrations (Spearman r = [− 0.64, − 0.63, – 0.62, − 0.69, − 0.67], p < 0.001, Fig. 8C shows example correlation for P2, head-unrestrained condition).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Fig. 8D shows the impact of poor gaze point depth estimation on measured vergence angle.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel D",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Moreover, there was high variability in gaze point depth among the validation marker presentations (see Fig. 8A for example calibration set) even though all markers were in the same plane, either 1 or 1.8 m away from the scene camera (marked as dashed gray lines in Fig. 8B and illustrated in panel A).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel A",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Comparing with Fig. 8B, there is an inverse relationship between the gaze point depth and parallax error.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel B",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Summary statistics for parallax error for data shown in Fig. 9*\n\n\n\n\n\n\n\n\n\n\nCalibration\nP2 Head-restrainedMean ± std / median [q25 q75]\nP2 Head-unrestrained\nP1 with Eyelink\nP2 with Eyelink\n\n\n\n\n\n\n\n5-point\n0.076 [0.057 0.109]\n0.020 [0.013 0.068]\n\n0.022 ± 0.006\n\n\n0.008 ± 0.008\n\n\n\n9-point\n0.044 [0.028 0.070]\n0.029 [0.016 0.076]\n\n0.016 ± 0.006\n\n0.008 [0.004 0.014]\n\n\n13-point\n0.013 [0.009 0.044]\n0.042 [0.029 0.091]\n\n0.022 ± 0.006\n\n0.008 [0.003 0.011]\n\n\nStar\n0.130 [0.102 0.162]\n0.137 [0.103 0.189]\n\n0.009 ± 0.005\n\n0.020 [0.014 0.022]\n\n\naVOR\n\n0.022 [0.012 0.065]\n\n\n\n\n\nOpen in a new tab\n\nThe bold headings correspond to the numbers in bold in the cells where mean/std values were used\n\n*For data distributed normally, mean, and standard deviation are included (bold text).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Summary statistics for parallax error for data shown in Fig. 9*.",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Summary statistics for parallax error for data shown in Fig. 9*.",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "The measurement error on screen was calculated in three ways (illustrated in Fig. 9A, see Methods 4.8.2) for both head-restrained (Fig. 9B) and head-unrestrained conditions (see Table 5 for comparison of all conditions/participants).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel A",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 9",
          "panel": "Panel B",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 9",
          "panel": "Panel B",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 9",
          "panel": "Panel B",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "First, the error is calculated as the distance between the onscreen intersection point of the cyclopean gaze ray and the respective fixation target locations of the validation set under each calibration (Fig. 9, orange).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Second, we repeated the calculation for the right and left eye gaze rays individually (Fig. 9, blue and red).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Finally, we estimated the distance between the screen intersection points of the right and left eye (Fig. 9, seafoam).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "To compare our errors to those measured by the EyeLink, we also performed a homography transformation between the camera image and the display screen in pixels and scaled to physical distances using the pixel per meter constant (see “Measured error in 2D: scene camera image and display screen“ section in Methods and “Error in 2D: scene camera image and display screen“ section in Results, Fig. 9C).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel C",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Errors calculated using this method are superimposed with the cyclopean error in orange (orange bars from Fig. 9B/Table 5, “Cyclopean” columns).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel B",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Importantly, the relative rotation angles for each eye are invariant to the reference coordinate frame, and are thus not affected by the choice of calibration (Fig. 1, column 1).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "As the calibration matrix just transforms the reference frame of the eye camera to the reference frame of the scene camera (Fig. 1, column 2), the eye rotation will be the same in both reference frames.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For each recording session, we examined saccade amplitude for the rotation of the cyclopean gaze, the left, and the right eye (akin to the illustration in Fig. 9A, see “Measured error in 2D: scene camera image and display screen“ and “Saccade amplitude error estimation: Intermittent eye motion between two target locations“ sections in Methods).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel A",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "Thus, in Fig. 10 we show data for a single calibration (nine-point).",
      "figure_references": [
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        }
      ]
    },
    {
      "sentence": "There was significant variation in the median saccade size across the sessions (4° and 10°, Fig. 10A & B).",
      "figure_references": [
        {
          "figure_number": "Figure 10",
          "panel": "Panel A",
          "figure_key": "figure_10"
        }
      ]
    },
    {
      "sentence": "Overall, eye-in-head angular velocity exceeded head-in-space velocity by 15–17 %, for a participant viewing a distant (180 cm) target (Fig. 11E, F).",
      "figure_references": [
        {
          "figure_number": "Figure 11",
          "panel": "Panel E",
          "figure_key": "figure_11"
        }
      ]
    },
    {
      "sentence": "Overall, eye-in-head angular velocity exceeded head-in-space velocity by 15–17 %, for a participant viewing a distant (180 cm) target (Fig. 11E, F).",
      "figure_references": [
        {
          "figure_number": "Figure 11",
          "panel": "Panel E",
          "figure_key": "figure_11"
        }
      ]
    },
    {
      "sentence": "These experiments span several stages of the eye-tracking pipeline (Fig. 1) and experimenters should assess and mitigate those that are relevant to their specific experiment type.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "First, motion of the eye cameras relative to each other and the rest of the device may not only introduce artificial movement of the eyes but invalidate the coordinate frame transformations computed during calibration (Fig. 4).",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Second, the choice of a calibration grid can affect eye orientation estimates (Fig. 5) that cannot be parsed without a ground truth reference.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "The headset was tightly secured to the head using an adjustable rigid plastic band (Fig. 3) to ensure that there was no movement between it and the participant.",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "The walk and run conditions yielded the least eye camera motion for both eyes (~ 12% of total movement), while jumping yielded over 50% more eye camera motion than walking, particularly in the right eye (Fig. 4E).",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "While walking had the lowest vertical accelerations (Fig. 4F), running was associated with higher accelerations.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "The headset was tightly secured to the head using an adjustable rigid plastic band (Fig. 3) to ensure that there was no movement between it and the participant.",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "The walk and run conditions yielded the least eye camera motion for both eyes (~ 12% of total movement), while jumping yielded over 50% more eye camera motion than walking, particularly in the right eye (Fig. 4E).",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel E",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "While walking had the lowest vertical accelerations (Fig. 4F), running was associated with higher accelerations.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        },
        {
          "figure_number": "Figure 4",
          "panel": "Panel F",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Gaze estimation noise\nIn the second set of experiments, we evaluated Pupil Core’s performance in three different spatial domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the 3D space of the scene camera (not referenced to the external world, Fig. 1, column 3); and (3) relative to the physical marker location in 3D external world (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Further, we find that while the accuracy of the cyclopean gaze position may still appear relatively high, eye rotation estimation accuracy tends to be very poor and leads to viewing distance underestimation (Fig. 8).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "While this information can inform certain experiments, our data demonstrate that it does not provide sufficient information to help the experimenter choose an optimal calibration, predict the amount or variability of assessed parallax error, or reliably calculate eye rotation amplitudes (e.g., Fig. 10).",
      "figure_references": [
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        }
      ]
    },
    {
      "sentence": "For a single fixation target, the misestimation of the binocular gaze point away from the calibration plane results in incorrect vergence angle between the gaze rays of the two eyes, a positional shift between monocular points of regard on screen (and thus an increase in distance between the projections of the two eyes), and parallax error on the viewing screen (i.e., the calibration plane, see Fig. 8E for illustration).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel E",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "This variability in depth estimation of the binocular gaze point leads to a variable vergence angle estimates, large offsets of the eye position estimate relative to the marker (Fig. 8), and non-constant bias in parallax error (Table 4).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "In the second set of experiments, we evaluated Pupil Core’s performance in three different spatial domains: (1) the 2D scene camera image (Fig. 1, column 2); (2) the 3D space of the scene camera (not referenced to the external world, Fig. 1, column 3); and (3) relative to the physical marker location in 3D external world (Fig. 1, column 4).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        },
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Further, we find that while the accuracy of the cyclopean gaze position may still appear relatively high, eye rotation estimation accuracy tends to be very poor and leads to viewing distance underestimation (Fig. 8).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "While this information can inform certain experiments, our data demonstrate that it does not provide sufficient information to help the experimenter choose an optimal calibration, predict the amount or variability of assessed parallax error, or reliably calculate eye rotation amplitudes (e.g., Fig. 10).",
      "figure_references": [
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        }
      ]
    },
    {
      "sentence": "For a single fixation target, the misestimation of the binocular gaze point away from the calibration plane results in incorrect vergence angle between the gaze rays of the two eyes, a positional shift between monocular points of regard on screen (and thus an increase in distance between the projections of the two eyes), and parallax error on the viewing screen (i.e., the calibration plane, see Fig. 8E for illustration).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel E",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "This variability in depth estimation of the binocular gaze point leads to a variable vergence angle estimates, large offsets of the eye position estimate relative to the marker (Fig. 8), and non-constant bias in parallax error (Table 4).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "First, we found that depending on the calibration choreography, gaze orientation can vary significantly for a representative fixation estimate (Fig. 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Further, the choice of calibration could have a significant effect on the estimation of the gaze point in depth, and therefore vergence angle (Fig. 8).",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "The compact nine-point star calibration had by far the most different outcomes in terms of camera position estimate (Fig. 5), variability in accuracy across the locations of the validation grid (Fig. 7), and position estimation error for each eye (and the distance between them, Fig. 9).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 9",
          "panel": "",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "This latter may be related to the significant gaze depth underestimates and resultant vergence angle overestimates (Fig. 8) for the star calibration.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Notably, although the compact star may seem like the least desirable calibration based on these metrics, it was also one that had the least variable gaze depth estimates (Fig. 8C) and may thus be easiest to correct using a spatial transformation.",
      "figure_references": [
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        },
        {
          "figure_number": "Figure 8",
          "panel": "Panel C",
          "figure_key": "figure_8"
        }
      ]
    },
    {
      "sentence": "Generally, the use of a homography transformation, when possible, may improve eye position accuracy (Fig. 9C).",
      "figure_references": [
        {
          "figure_number": "Figure 9",
          "panel": "Panel C",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 9",
          "panel": "Panel C",
          "figure_key": "figure_9"
        },
        {
          "figure_number": "Figure 9",
          "panel": "Panel C",
          "figure_key": "figure_9"
        }
      ]
    },
    {
      "sentence": "However, the mask to which the printed eyes were attached was rigid plastic (thus not allowing for any slippage due to skin deformation) and the headset was secured very tightly to the head using a ratcheting restraint (Fig. 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "We observed similar amplitude variability between the two devices for both participants, though the mean amplitudes did tend to differ between the two (Fig. 10).",
      "figure_references": [
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        },
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        },
        {
          "figure_number": "Figure 10",
          "panel": "",
          "figure_key": "figure_10"
        }
      ]
    }
  ],
  "extraction_stats": {
    "figures_count": 10,
    "claims_count": 134,
    "images_downloaded": 10,
    "tables_filtered": 57
  }
}