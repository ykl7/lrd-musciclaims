{
  "paper_id": "PMC11270031",
  "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11270031/",
  "figures": {
    "figure_1": {
      "figure_number": "Figure 1",
      "title": "Figure 1.",
      "caption": "PRISMA flow chart of the systematic review process.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "fig1",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11270031/images/figure_1.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/eef296fc4124/fpsyg-15-1428732-g001.jpg"
    },
    "figure_2": {
      "figure_number": "Figure 2",
      "title": "Figure 2.",
      "caption": "Forest plot of effect sizes.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "fig2",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11270031/images/figure_2.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/06fc335b0397/fpsyg-15-1428732-g002.jpg"
    },
    "figure_3": {
      "figure_number": "Figure 3",
      "title": "Figure 3.",
      "caption": "Funnel plot of effect sizes.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "fig3",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC11270031/images/figure_3.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/4414/11270031/18e5f1d0e54a/fpsyg-15-1428732-g003.jpg"
    }
  },
  "claims": [
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A total of 6,299 citations were found (see Figure 1 for a flow chart of the systematic search process).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Of these studies, 12 were selected for inclusion (see Figure 1 for reasons for exclusion).",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Study, learning style group, condition (if more than one), measure (if more than one), subgroups (if any)\n\n\nNumber of participants\n\n\nHedges’ g\n\n\nVariance of Hedges’ g\n\n\n\n\n\nAslaksen and Lorås (2019), auditory\n9\n−1.25\n0.44\n\n\n\nAslaksen and Lorås (2019), visual\n13\n0.85\n0.30\n\n\n\nBurns (n.d.), auditory\n7\n−0.10\n0.04\n\n\n\nBurns (n.d.), visual\n30\n0.44\n0.01\n\n\n\nChen (2020), auditory\n41\n−0.08\n0.09\n\n\n\nChen (2020), read/write\n34\n0.26\n0.11\n\n\n\nChui et al. (\n\n2021)\n\n, auditory\n\n9\n1.56\n0.09\n\n\n\nChui et al. (\n\n2021)\n\n, visual\n\n9\n0.61\n0.04\n\n\n\nChen and Sun (\n\n2012)\n\n, verbal, interaction comparison\n\n73\n0.11\n0.06\n\n\n\nChen and Sun (\n\n2012)\n\n, visualizer, interactive treatment\n\n66\n0.08\n0.07\n\n\n\nCuevas and Dawson (2018), auditory\n118\n−2.87\n0.07\n\n\n\nCuevas and Dawson (2018), visual\n65\n2.13\n0.10\n\n\n\nGe (2021), auditory\n76\n−0.74\n0.06\n\n\n\nGe (2021), visual\n64\n1.20\n0.07\n\n\n\nHazra et al. (2013), verbal, engineering comprehension\n15\n−0.41\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history comprehension\n\n0.06\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recall\n−0.21\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recall\n\n0.13\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering recognition\n−0.44\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history recognition\n\n0.46\n0.24\n\n\n\nHazra et al. (2013), verbal, engineering transfer\n0.10\n0.24\n\n\n\nHazra et al. (\n\n2013)\n\n, verbal, history transfer\n\n0.04\n0.24\n\n\n\nHazra et al. (2013), visual, engineering comprehension\n124\n−0.10\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history comprehension\n\n0.43\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recall\n0.00\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recall\n\n0.25\n0.03\n\n\n\nHazra et al. (2013), visual, engineering recognition\n0.19\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history recognition\n\n0.30\n0.03\n\n\n\nHazra et al. (2013), visual, engineering transfer\n−0.04\n0.03\n\n\n\nHazra et al. (\n\n2013)\n\n, visual, history transfer\n\n0.16\n0.03\n\n\n\nKam et al. (\n\n2020)\n\n, auditory\n\n29\n0.76\n0.14\n\n\n\nKam et al. (\n\n2020)\n\n, visual\n\n31\n0.79\n0.13\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 1\n\n29\n0.78\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, auditory, week 2\n\n0.77\n0.02\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 1\n\n37\n0.80\n0.01\n\n\n\nKassaian (\n\n2007)\n\n, visual, week 2\n\n0.64\n0.02\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, auditory comprehension\n\n21\n0.26\n0.18\n\n\n\nLehmann and Seufert (2020), auditory recall\n−0.11\n0.18\n\n\n\nLehmann and Seufert (\n\n2020)\n\n, visual comprehension\n\n21\n1.04\n0.20\n\n\n\nLehmann and Seufert (2020), visual recall\n0.86\n0.19\n\n\n\nMoser and Zumbach (2018), verbalizer VVQ\n42\n0.45\n0.09\n\n\n\nMoser and Zumbach (2018), verbalizer SBLSQ\n40\n0.61\n0.10\n\n\n\nMoser and Zumbach (2018), visualizer VVQ\n82\n−0.03\n0.05\n\n\n\nMoser and Zumbach (2018), visualizer SBLSQ\n73\n−0.04\n0.05\n\n\n\nMoussa-Inaty et al. (2019), auditory\n31\n−0.25\n0.12\n\n\n\nMoussa-Inaty et al. (2019), visual\n30\n0.39\n0.13\n\n\n\nMujtaba et al. (2022), auditory OPT delayed\n40\n1.40\n0.12\n\n\n\nMujtaba et al. (2022), auditory OPT post\n1.35\n0.12\n\n\n\nMujtaba et al. (2022), auditory WT delayed\n1.73\n0.13\n\n\n\nMujtaba et al. (2022), auditory WT post\n1.36\n0.12\n\n\n\nMujtaba et al. (2022), visual OPT delayed\n40\n−0.53\n0.10\n\n\n\nMujtaba et al. (2022), visual OPT post\n−0.56\n0.10\n\n\n\nMujtaba et al. (2022), visual WT delayed\n−0.59\n0.10\n\n\n\nMujtaba et al. (2022), visual WT post\n−0.63\n0.10\n\n\n\nPapanagnou et al. (2016), auditory\n52\n−0.07\n0.34\n\n\n\nPapanagnou et al. (2016), kinesthetic\n62\n0.52\n0.07\n\n\n\nPapanagnou et al. (2016), visual\n48\n−0.27\n0.08\n\n\n\nRassaei (2018), auditory delayed production\n32\n2.58\n0.22\n\n\n\nRassaei (2018), auditory delayed recognition\n2.14\n0.19\n\n\n\nRassaei (2018), auditory post production\n2.02\n0.18\n\n\n\nRassaei (2018), auditory post recognition\n\n1.88\n0.17\n\n\n\nRassaei (2018), visual delayed production\n30\n−1.47\n0.16\n\n\n\nRassaei (2018), visual delayed recognition\n\n−1.23\n0.15\n\n\n\nRassaei (2018), visual post production\n−1.48\n0.16\n\n\n\nRassaei (2018), visual post recognition\n−1.35\n0.16\n\n\n\nRassaei (2019), auditory delayed OPT\n31\n1.59\n0.16\n\n\n\nRassaei (2019), auditory post OPT\n\n1.72\n0.17\n\n\n\nRassaei (2019), auditory delayed WT\n\n1.69\n0.17\n\n\n\nRassaei (2019), auditory post WT\n\n1.77\n0.17\n\n\n\nRassaei (2019), read/write delayed OPT\n30\n−0.34\n0.13\n\n\n\nRassaei (2019), read/write post OPT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write delayed WT\n\n−0.04\n0.13\n\n\n\nRassaei (2019), read/write post WT\n\n−0.28\n0.13\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup explanation\n10\n−0.70\n0.35\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup labelling\n0.12\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup problem solving\n0.08\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, analytic subgroup short recall\n−1.20\n0.40\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, explanation\n10\n−0.15\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, labeling\n−0.11\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, problem solving\n \n−0.37\n0.33\n\n\n\nRiding and Douglas (1993), verbalizer, wholist subgroup, short recall\n−0.17\n0.33\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup explanation\n10\n1.51\n0.44\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup labeling\n\n1.33\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup problem solving\n\n1.08\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, analytic subgroup short recall\n\n0.50\n0.34\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, explanation\n10\n1.11\n0.39\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, labeling\n\n1.05\n0.38\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, problem solving\n\n1.30\n0.41\n\n\n\nRiding and Douglas (1993), visualizer, wholist subgroup, short recall\n\n1.35\n0.42\n\n\n\nRogowsky et al. (2015), auditory time one\n21\n−0.25\n0.18\n\n\n\nRogowsky et al. (2015), auditory time two\n−0.24\n0.18\n\n\n\nRogowsky et al. (2015), visual time one\n20\n−0.11\n0.18\n\n\n\nRogowsky et al. (2015), visual time two\n−0.20\n0.18\n\n\n\nRogowsky et al. (2020) auditory\n12\n0.17\n0.03\n\n\n\nRogowsky et al. (2020) visual\n22\n−0.12\n0.02\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, auditory\n\n7\n2.03\n0.16\n\n\n\nTadayonifar et al. (\n\n2021)\n\n, read/write\n\n6\n2.63\n0.27\n\n\n\nOpen in a new tab\nLearning outcomes indicating a crossover effect as articulated in Pashler et al. (2008) in which at least two styles had higher learning outcomes with matched instruction are bolded.Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Each of the study effect sizes is shown in Table 2, and a forest plot is in Figure 2.",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "A funnel plot was generated using the “metafor” package in R (Viechtbauer, 2010; see Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "As indicated in Figure 1, five that had their full texts screened did not have sufficient statistics to calculate the effect sizes reported.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    }
  ],
  "extraction_stats": {
    "figures_count": 3,
    "claims_count": 65,
    "images_downloaded": 3,
    "tables_filtered": 75
  }
}