paper_id,claim,figure_id,title,caption,local_image_path,url
PMC7805959,Such a heterogeneous multi-robot team of a UAS and a USV is illustrated in Figure 1.,PMC7805959_figure_1,Figure 1.,DJI Inspire 1 (UAS) assisting EMILY (USV) using visual feedback.,./data/PMC7805959/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/2df2d4ad31f1/frobt-06-00042-g0001.jpg
PMC7805959,"This distance causes the spatial resolution of the USV to be very low as illustrated in Figure 2 implicating that fiducial markers encoding full pose (e.g., AprilTag) would not be visible.",PMC7805959_figure_2,Figure 2.,The spatial resolution of the object in the UAS view might be very low.,./data/PMC7805959/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/87e8bd7d76f7/frobt-06-00042-g0002.jpg
PMC7805959,The physical configuration of the problem is depicted in Figure 3.,PMC7805959_figure_3,Figure 3.,"The physical configuration of the problem. {I} is UAS 2D image coordinate system with axes Xi and Yi. F(t) is UAS video frame at time t with resolution w × h. a is UAS altitude above ground level. α is UAS camera angle from nadir. (x(t), y(t)) is the object's blob position, that is the centroid of the object's blob in the coordinate frame {I}. θ(t) is the object's blob orientation, that is the angle between the object's blob heading and horizontal line (axis Xi) in the coordinate frame {I}.",./data/PMC7805959/images/figure_3.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/04fcf91a4880/frobt-06-00042-g0003.jpg
PMC7805959,An example of the relative pose influence on the brightness is when the object that faces the UAS with its non-illuminated side changes its pose in a way that it now faces the UAS with its sun illuminated side as shown in Figure 4.,PMC7805959_figure_4,Figure 4.,Object's blob brightness may vary significantly in the video frames F(t) with varying t because of the outdoor environmental conditions.,./data/PMC7805959/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/13b750877018/frobt-06-00042-g0004.jpg
PMC7805959,Another example is when the sun is in the field of view of the UAS's camera causing the white balance distortion in the video frames as can be seen in Figure 5.,PMC7805959_figure_5,Figure 5.,"The illumination conditions may change with the relative position of the UAS, object, and sun.",./data/PMC7805959/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/abeedfac4bd2/frobt-06-00042-g0005.jpg
PMC7805959,"Open in a new tab
Object's blob brightness may vary significantly in the video frames F(t) with varying t because of the outdoor environmental conditions.Figure 5.",PMC7805959_figure_5,Figure 5.,"The illumination conditions may change with the relative position of the UAS, object, and sun.",./data/PMC7805959/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/abeedfac4bd2/frobt-06-00042-g0005.jpg
PMC7805959,"If the object moves away from the UAS, its size will decrease and vice versa as can be seen in Figure 6.",PMC7805959_figure_6,Figure 6.,Object's blob size may differ significantly in the video frames F(t) with varying t because of the oblique view angle.,./data/PMC7805959/images/figure_6.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/8347f66a15e2/frobt-06-00042-g0006.jpg
PMC7805959,"Taking a cylindrical object such as a USV as an example, if such object is facing toward the UAS, its blob will be circular (inertia ratio will be high), however, If it is turned sideways, its blob will be elliptical (inertia ratio will be low) as shown in Figure 7.",PMC7805959_figure_7,Figure 7.,The ratio of the minimum inertia to maximum inertia of the object's blob may differ significantly in the video frames F(t) with varying t because of the oblique view angle.,./data/PMC7805959/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/14b669b747d1/frobt-06-00042-g0007.jpg
PMC7805959,"In the extreme case, the object's blob might be featureless due to the large distance between the object and the UAS causing the spatial resolution might be very low as illustrated in Figure 2.",PMC7805959_figure_2,Figure 2.,The spatial resolution of the object in the UAS view might be very low.,./data/PMC7805959/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/87e8bd7d76f7/frobt-06-00042-g0002.jpg
PMC7805959,"Second, the UAS motion causes the background to move as well as shown in Figure 8.",PMC7805959_figure_8,Figure 8.,Moving UAS causes the stationary background to appear moving in the video frames.,./data/PMC7805959/images/figure_8.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/eb3033704261/frobt-06-00042-g0008.jpg
PMC7805959,This problem can be alleviated by using Gaussian blur convolution filter to diffuse the color of the smaller color blobs into the bigger color blobs as illustrated in Figure 9.,PMC7805959_figure_9,Figure 9.,The color of the object might not be uniform which can be alleviated by Gaussian blur convolution filter.,./data/PMC7805959/images/figure_9.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/1604a4e0cb45/frobt-06-00042-g0009.jpg
PMC7805959,"The Gaussian filter can be applied by convolving the original image with a Gaussian kernel as follows:
Figure 9.",PMC7805959_figure_9,Figure 9.,The color of the object might not be uniform which can be alleviated by Gaussian blur convolution filter.,./data/PMC7805959/images/figure_9.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/1604a4e0cb45/frobt-06-00042-g0009.jpg
PMC7805959,This processed is schematized in the left flowchart in Figure 10.,PMC7805959_figure_10,Figure 10.,Flowcharts for the thresholding and histogramming algorithms.,./data/PMC7805959/images/figure_10.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/dfe118977898/frobt-06-00042-g0010.jpg
PMC7805959,An example of a binary threshold map can be seen on the left side of Figure 11.,PMC7805959_figure_11,Figure 11.,"Erosion and dilation filters can be applied to the binary threshold map (Left) to filter out noise, smooth shapes, and fill-in holes (Right).",./data/PMC7805959/images/figure_11.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/0d9bcce5bd15/frobt-06-00042-g0011.jpg
PMC7805959,"The binary threshold map is created as follows:
Figure 11.",PMC7805959_figure_11,Figure 11.,"Erosion and dilation filters can be applied to the binary threshold map (Left) to filter out noise, smooth shapes, and fill-in holes (Right).",./data/PMC7805959/images/figure_11.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/0d9bcce5bd15/frobt-06-00042-g0011.jpg
PMC7805959,An example of a binary threshold map after the application of erosion and dilation can be seen on the right side of Figure 11.,PMC7805959_figure_11,Figure 11.,"Erosion and dilation filters can be applied to the binary threshold map (Left) to filter out noise, smooth shapes, and fill-in holes (Right).",./data/PMC7805959/images/figure_11.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/0d9bcce5bd15/frobt-06-00042-g0011.jpg
PMC7805959,This processed is schematized in the right flowchart in Figure 10.,PMC7805959_figure_10,Figure 10.,Flowcharts for the thresholding and histogramming algorithms.,./data/PMC7805959/images/figure_10.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/dfe118977898/frobt-06-00042-g0010.jpg
PMC7805959,"The result is a greyscale map of the same dimensions as F(t), where each pixel's value signifies how much this pixel's hue is represented in the histogram as shown in Figure 12.",PMC7805959_figure_12,Figure 12.,The backprojection of the object's histogram model and subsequent application of CamShift.,./data/PMC7805959/images/figure_12.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/1d8f460f0abe/frobt-06-00042-g0012.jpg
PMC7805959,The algorithm eventually converges to a local maximum density area and tracks it as can be seen in Figure 12.,PMC7805959_figure_12,Figure 12.,The backprojection of the object's histogram model and subsequent application of CamShift.,./data/PMC7805959/images/figure_12.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/1d8f460f0abe/frobt-06-00042-g0012.jpg
PMC7805959,An example of the fitted ellipse and the estimated orientation can be seen in Figure 12.,PMC7805959_figure_12,Figure 12.,The backprojection of the object's histogram model and subsequent application of CamShift.,./data/PMC7805959/images/figure_12.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/1d8f460f0abe/frobt-06-00042-g0012.jpg
PMC7805959,EMILY can be seen in Figure 1.,PMC7805959_figure_1,Figure 1.,DJI Inspire 1 (UAS) assisting EMILY (USV) using visual feedback.,./data/PMC7805959/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/2df2d4ad31f1/frobt-06-00042-g0001.jpg
PMC7805959,DJI Inspire 1 can be seen in Figure 1.,PMC7805959_figure_1,Figure 1.,DJI Inspire 1 (UAS) assisting EMILY (USV) using visual feedback.,./data/PMC7805959/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/2df2d4ad31f1/frobt-06-00042-g0001.jpg
PMC7805959,The trial in progress can be seen in Figure 1.,PMC7805959_figure_1,Figure 1.,DJI Inspire 1 (UAS) assisting EMILY (USV) using visual feedback.,./data/PMC7805959/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/2df2d4ad31f1/frobt-06-00042-g0001.jpg
PMC7805959,The view from the UAS looking at the USV during this trial can be seen in Figure 2.,PMC7805959_figure_2,Figure 2.,The spatial resolution of the object in the UAS view might be very low.,./data/PMC7805959/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/87e8bd7d76f7/frobt-06-00042-g0002.jpg
PMC7805959,This is illustrated in Figure 2 where a USV's blob is circular making it hard to estimate the orientation by shape analysis.,PMC7805959_figure_2,Figure 2.,The spatial resolution of the object in the UAS view might be very low.,./data/PMC7805959/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/87e8bd7d76f7/frobt-06-00042-g0002.jpg
PMC7805959,"For example, the camera may face the sun distorting white balance in the entire view as can be seen in Figure 5.",PMC7805959_figure_5,Figure 5.,"The illumination conditions may change with the relative position of the UAS, object, and sun.",./data/PMC7805959/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/abeedfac4bd2/frobt-06-00042-g0005.jpg
PMC7805959,Another example is presented in Figure 4 illustrating how significantly can illumination conditions change during a single run.,PMC7805959_figure_4,Figure 4.,Object's blob brightness may vary significantly in the video frames F(t) with varying t because of the outdoor environmental conditions.,./data/PMC7805959/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/b541/7805959/13b750877018/frobt-06-00042-g0004.jpg
