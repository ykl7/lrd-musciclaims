paper_id,claim,figure_id,title,caption,local_image_path,url
PMC11095461,"Furthermore, because singing and speaking exist on a broader “musi-linguistic” spectrum including forms such as instrumental music and poetry recitation (41–43), we collected four types of recordings to capture variation across this spectrum: (i) singing, (ii) recitation of the sung lyrics, (iii) spoken description of the song, and (iv) instrumental version of the sung melody (Fig. 1).",PMC11095461_figure_1,Fig. 1. Example excerpts of the four recording types collected in this study arranged in a musi-linguistic continuum from instrumental music to spoken language.,"Spectrograms [x axis, time (in seconds); y axis, frequency (in hertz)] of the four types of recordings are displayed on the right-hand side (excerpts of author Savage performing/describing “Twinkle Twinkle Little Star,” using a piano for the instrumental version). Blue dashed lines show the schematic illustration of the mapping between the audio signal and acoustic units (here syllables/notes). For this Registered Report, we focus our confirmatory hypothesis only on comparisons between singing and spoken description (red rectangles), with recited and instrumental versions saved for post hoc exploratory analysis.",./data/PMC11095461/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/42dfde229482/sciadv.adm9797-f1.jpg
PMC11095461,"Question
Hypothesis
Sampling plan
Analysis plan
Rationale for deciding the test sensitivity
Interpretation given different outcomes
Theory that could be shown wrong by the outcomes
Actual outcome



Are any acoustic features reliably different between song and speech across cultures?
1) Song uses higher pitch than speech

n = 81 pairs of audio recordings of song/speech, with each pair sung/spoken by the same person (Fig. 2).",PMC11095461_figure_2,Fig. 2. Visualization of the diversity of the primary sample of 300 audio recordings of singing/speaking/recitation/instrumental melodies.,"Map of the linguistic varieties spoken by our 75 coauthors as first/heritage languages (A). (Note: 6 of the original 81 planned coauthors were unable to complete the recording and annotation process compared to our initially planned sample; compare fig. S1 for the original map of 81 linguistic varieties). Each circle represents a coauthor singing and speaking in their first (L1) or heritage language. The geographic coordinates represent their hometown where they learned that language. In cases when the language name preferred by that coauthor (ethnonym) differs from the L1 language name in the standardized classification in the Glottolog (47), the ethnonym is listed first, followed by the Glottolog name in round brackets. Language family classifications (in bold) are based on Glottolog. Square brackets indicate geographic locations for languages represented by more than one coauthor. Atlantic-Congo, Indo-European, and Sino-Tibetan languages are further grouped by genus defined by the World Atlas of Language Structures (48). The word clouds outline the most common textual content of English translations of the song lyrics (B) and spoken descriptions (C) provided by our 75 coauthors (larger text indicates words that appear more frequently).",./data/PMC11095461/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/98aa95a54c26/sciadv.adm9797-f2.jpg
PMC11095461,"We did not collect standardized information about the function/context of songs, but the word clouds of lyrics translated to English (compare Fig. 2B) may provide an idea about what the songs are about, as do the English translations of the spoken descriptions (all available with other data at https://osf.io/mzxc8).",PMC11095461_figure_2,Fig. 2. Visualization of the diversity of the primary sample of 300 audio recordings of singing/speaking/recitation/instrumental melodies.,"Map of the linguistic varieties spoken by our 75 coauthors as first/heritage languages (A). (Note: 6 of the original 81 planned coauthors were unable to complete the recording and annotation process compared to our initially planned sample; compare fig. S1 for the original map of 81 linguistic varieties). Each circle represents a coauthor singing and speaking in their first (L1) or heritage language. The geographic coordinates represent their hometown where they learned that language. In cases when the language name preferred by that coauthor (ethnonym) differs from the L1 language name in the standardized classification in the Glottolog (47), the ethnonym is listed first, followed by the Glottolog name in round brackets. Language family classifications (in bold) are based on Glottolog. Square brackets indicate geographic locations for languages represented by more than one coauthor. Atlantic-Congo, Indo-European, and Sino-Tibetan languages are further grouped by genus defined by the World Atlas of Language Structures (48). The word clouds outline the most common textual content of English translations of the song lyrics (B) and spoken descriptions (C) provided by our 75 coauthors (larger text indicates words that appear more frequently).",./data/PMC11095461/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/98aa95a54c26/sciadv.adm9797-f2.jpg
PMC11095461,We compared the following six acoustic features (Fig. 3; compare the “Features” section in the Supplementary Materials for details) between song and speech for our main confirmatory analyses:.,PMC11095461_figure_3,"Fig. 3. Schematic illustration of the six features analyzed for confirmatory analysis, using a recording of author Savage singing the first two phrases of “Twinkle Twinkle Little Star” as an example.","Onset and breathing annotations are based on the segmented texts displayed on the top of the spectrogram. The y axis is adjusted to emphasize the f0 contour, so note that the spectral centroid information is not fully captured (e.g., high spectral centroid due to the consonant). The bottom figure shows pitch stability (rate of change of f0 or derivative of the f0 contour equivalently) of the sung f0.",./data/PMC11095461/images/figure_3.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/c9d0e2cce5dd/sciadv.adm9797-f3.jpg
PMC11095461,The results of the confirmatory hypothesis testing with 73 recording sets confirmed five of our six predictions (Fig. 4 and table S1; all P < 1 × 10−5).,PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics.",PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"Note that the colors of data points indicate language families, which are coded the same as in Fig. 2.",PMC11095461_figure_2,Fig. 2. Visualization of the diversity of the primary sample of 300 audio recordings of singing/speaking/recitation/instrumental melodies.,"Map of the linguistic varieties spoken by our 75 coauthors as first/heritage languages (A). (Note: 6 of the original 81 planned coauthors were unable to complete the recording and annotation process compared to our initially planned sample; compare fig. S1 for the original map of 81 linguistic varieties). Each circle represents a coauthor singing and speaking in their first (L1) or heritage language. The geographic coordinates represent their hometown where they learned that language. In cases when the language name preferred by that coauthor (ethnonym) differs from the L1 language name in the standardized classification in the Glottolog (47), the ethnonym is listed first, followed by the Glottolog name in round brackets. Language family classifications (in bold) are based on Glottolog. Square brackets indicate geographic locations for languages represented by more than one coauthor. Atlantic-Congo, Indo-European, and Sino-Tibetan languages are further grouped by genus defined by the World Atlas of Language Structures (48). The word clouds outline the most common textual content of English translations of the song lyrics (B) and spoken descriptions (C) provided by our 75 coauthors (larger text indicates words that appear more frequently).",./data/PMC11095461/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/98aa95a54c26/sciadv.adm9797-f2.jpg
PMC11095461,We performed two exploratory analyses using automated methods to investigate (i) the reproducibility of our findings with another corpus and (ii) the applicability of automated methods to substitute data extraction processes involving manual work (Fig. 6; compare the “Exploring recording representativeness and automated scalability” section in the Supplementary Materials).,PMC11095461_figure_6,Fig. 6. Rerunning the analyses on four different samples using different fundamental frequency extraction methods.,"Three features could be directly compared between the different samples: pitch height (A), pitch stability (B), and timbral brightness (C). The following samples were compared: (i) Our full sample (matched song and speech recordings from our 75 coauthors); (ii) Hilton et al.’s (26) full sample (matched song and speech recordings from 209 individuals); (iii) a subsample of our 14 coauthors singing/speaking in English, Spanish, Mandarin, Kannada, and Polish; and (iv) a subsample of Hilton et al.’s (26) 122 participants also singing/speaking in English, Spanish, Mandarin, Kannada, and Polish). “SA” means that f0 values are extracted in a semiautomated manner (compare the “Pitch height” section in the Supplementary Materials), while “FA” means they were exactly in a fully automated manner (using the pYIN algorithm). The visualization follows the same convention as in Figs. 4 and 5. However, Hilton et al.’s (26) dataset contains languages that are not in our dataset. Therefore, slightly different color mapping was applied (compare fig. S16). Note that some large effect sizes (D > 3.5) in the pitch height of our original analysis (i.e., full, SA, 20 s) are not observed in the automated analysis (i.e., full, FA, full length). This is due to estimation errors in the automated analyses. When erroneous f0 values of pYIN are very high in spoken description or very low in singing, relative effects become smaller than semiautomated methods that remove these errors.",./data/PMC11095461/images/figure_6.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/0b2a9cd9990b/sciadv.adm9797-f6.jpg
PMC11095461,"Similar to Fig. 5, mean values of each feature per recording can be found in the Supplementary Materials (figs.",PMC11095461_figure_5,Fig. 5. Mean values of acoustic features arranged along a “musi-linguistic continuum” from instrumental melodies to spoken descriptions.,"This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics. The cent scale of f0 is converted from Hertz, where 440 Hz corresponds to 0 cents and an octave interval equals 1200 cents. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2. The horizontal lines in the violin plots indicate the median.",./data/PMC11095461/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/07090a7094bd/sciadv.adm9797-f5.jpg
PMC11095461,"However, only three to four f0 slopes (equal to the number of “phrases” or intervals from the first onset after a break and to the next break; compare Fig. 3) are, on average, included in the 20-s length recording of singing and spoken description, respectively, and so it is possible that this failed prediction could be due to the relatively more limited amount of data available for this feature.",PMC11095461_figure_3,"Fig. 3. Schematic illustration of the six features analyzed for confirmatory analysis, using a recording of author Savage singing the first two phrases of “Twinkle Twinkle Little Star” as an example.","Onset and breathing annotations are based on the segmented texts displayed on the top of the spectrogram. The y axis is adjusted to emphasize the f0 contour, so note that the spectral centroid information is not fully captured (e.g., high spectral centroid due to the consonant). The bottom figure shows pitch stability (rate of change of f0 or derivative of the f0 contour equivalently) of the sung f0.",./data/PMC11095461/images/figure_3.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/c9d0e2cce5dd/sciadv.adm9797-f3.jpg
PMC11095461,"Last, we also checked the average trend of f0 contours segmented by onset and break annotations (compare Fig. 7).",PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,"Thus, on average, spoken pitch contours tend to descend more than sung pitch contours, explaining our failure to confirm our prediction that their contours would display similar pitch declination (compare Fig. 5).",PMC11095461_figure_5,Fig. 5. Mean values of acoustic features arranged along a “musi-linguistic continuum” from instrumental melodies to spoken descriptions.,"This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics. The cent scale of f0 is converted from Hertz, where 440 Hz corresponds to 0 cents and an octave interval equals 1200 cents. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2. The horizontal lines in the violin plots indicate the median.",./data/PMC11095461/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/07090a7094bd/sciadv.adm9797-f5.jpg
PMC11095461,"We also noticed that vocalizers sometimes end their utterance by raising pitch in their spoken description recordings (and lyrics recitation as well), causing a slight rise at the end of the averaged f0 contour of spoken description (and lyrics recitation; compare Fig. 7).",PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,"The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.Furthermore, the width of SEs around the mean contour (compare Fig. 7) suggests that spoken description and lyrics recitation have more homogeneous variations of contours than song and instrumental.",PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.,PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,"Furthermore, the width of SEs around the mean contour (compare Fig. 7) suggests that spoken description and lyrics recitation have more homogeneous variations of contours than song and instrumental.",PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,"Main confirmatory predictions and their robustness
Our analyses strongly support five of our six predictions across an unprecedentedly diverse global sample of music/speech recordings: (i) Song uses higher pitch than speech, (ii) song is slower than speech, (iii) song uses more stable pitches than speech, (iv) song and speech use similar timbral brightness, and (v) song and speech use similar sized pitch intervals (Fig. 4).",PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"Furthermore, the first three features display a shift of distribution along the musi-linguistic continuum, with instrumental melodies tending to use even higher and more stable pitches than song and lyric recitation tending to fall in between conversational speech and song (Fig. 5).",PMC11095461_figure_5,Fig. 5. Mean values of acoustic features arranged along a “musi-linguistic continuum” from instrumental melodies to spoken descriptions.,"This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics. The cent scale of f0 is converted from Hertz, where 440 Hz corresponds to 0 cents and an octave interval equals 1200 cents. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2. The horizontal lines in the violin plots indicate the median.",./data/PMC11095461/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/07090a7094bd/sciadv.adm9797-f5.jpg
PMC11095461,"Figure 4 shows many exceptions for four of the five features: for example, Parselelo (Kiswahili speaker) sang with a lower pitch than he spoke, and Ozaki (Japanese speaker) used slightly more stable pitches when speaking than singing, while many recording sets had examples where differences in sung versus spoken timbre or interval size were substantially larger than our designated SESOI.",PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"Instead of our predicted similarities, our exploratory analyses suggest that, while both song and speech contours tend to decline toward the end of a breath, they tend to do so in different ways: song first rising before falling to end near the same height as the beginning, speech first descending before briefly rising at the end (Fig. 7).",PMC11095461_figure_7,Fig. 7. Averaged f0 contours.,"f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",./data/PMC11095461/images/figure_7.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/34c5e5c44f32/sciadv.adm9797-f7.jpg
PMC11095461,"Analysis of Hilton et al.’s (26) dataset of field recordings also supplemented our findings, producing qualitatively identical conclusions, regardless of the precise analysis methods or specific sample/subsample used (Fig. 6).",PMC11095461_figure_6,Fig. 6. Rerunning the analyses on four different samples using different fundamental frequency extraction methods.,"Three features could be directly compared between the different samples: pitch height (A), pitch stability (B), and timbral brightness (C). The following samples were compared: (i) Our full sample (matched song and speech recordings from our 75 coauthors); (ii) Hilton et al.’s (26) full sample (matched song and speech recordings from 209 individuals); (iii) a subsample of our 14 coauthors singing/speaking in English, Spanish, Mandarin, Kannada, and Polish; and (iv) a subsample of Hilton et al.’s (26) 122 participants also singing/speaking in English, Spanish, Mandarin, Kannada, and Polish). “SA” means that f0 values are extracted in a semiautomated manner (compare the “Pitch height” section in the Supplementary Materials), while “FA” means they were exactly in a fully automated manner (using the pYIN algorithm). The visualization follows the same convention as in Figs. 4 and 5. However, Hilton et al.’s (26) dataset contains languages that are not in our dataset. Therefore, slightly different color mapping was applied (compare fig. S16). Note that some large effect sizes (D > 3.5) in the pitch height of our original analysis (i.e., full, SA, 20 s) are not observed in the automated analysis (i.e., full, FA, full length). This is due to estimation errors in the automated analyses. When erroneous f0 values of pYIN are very high in spoken description or very low in singing, relative effects become smaller than semiautomated methods that remove these errors.",./data/PMC11095461/images/figure_6.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/0b2a9cd9990b/sciadv.adm9797-f6.jpg
PMC11095461,"Our analyses strongly support five of our six predictions across an unprecedentedly diverse global sample of music/speech recordings: (i) Song uses higher pitch than speech, (ii) song is slower than speech, (iii) song uses more stable pitches than speech, (iv) song and speech use similar timbral brightness, and (v) song and speech use similar sized pitch intervals (Fig. 4).",PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"In general, many features followed the predicted “musi-linguistic continuum” with instrumental music and spoken conversation most extreme (e.g., most/least stable pitches respectively), with song and lyric recitation occupying intermediate positions (Fig. 5).",PMC11095461_figure_5,Fig. 5. Mean values of acoustic features arranged along a “musi-linguistic continuum” from instrumental melodies to spoken descriptions.,"This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics. The cent scale of f0 is converted from Hertz, where 440 Hz corresponds to 0 cents and an octave interval equals 1200 cents. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2. The horizontal lines in the violin plots indicate the median.",./data/PMC11095461/images/figure_5.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/07090a7094bd/sciadv.adm9797-f5.jpg
PMC11095461,"These differences were consistent even when analyzed using matching subsamples speaking the same languages and using the same fully automated analysis methods (Fig. 6), suggesting that they are not due to differences in the sample of languages or analysis methods we chose.",PMC11095461_figure_6,Fig. 6. Rerunning the analyses on four different samples using different fundamental frequency extraction methods.,"Three features could be directly compared between the different samples: pitch height (A), pitch stability (B), and timbral brightness (C). The following samples were compared: (i) Our full sample (matched song and speech recordings from our 75 coauthors); (ii) Hilton et al.’s (26) full sample (matched song and speech recordings from 209 individuals); (iii) a subsample of our 14 coauthors singing/speaking in English, Spanish, Mandarin, Kannada, and Polish; and (iv) a subsample of Hilton et al.’s (26) 122 participants also singing/speaking in English, Spanish, Mandarin, Kannada, and Polish). “SA” means that f0 values are extracted in a semiautomated manner (compare the “Pitch height” section in the Supplementary Materials), while “FA” means they were exactly in a fully automated manner (using the pYIN algorithm). The visualization follows the same convention as in Figs. 4 and 5. However, Hilton et al.’s (26) dataset contains languages that are not in our dataset. Therefore, slightly different color mapping was applied (compare fig. S16). Note that some large effect sizes (D > 3.5) in the pitch height of our original analysis (i.e., full, SA, 20 s) are not observed in the automated analysis (i.e., full, FA, full length). This is due to estimation errors in the automated analyses. When erroneous f0 values of pYIN are very high in spoken description or very low in singing, relative effects become smaller than semiautomated methods that remove these errors.",./data/PMC11095461/images/figure_6.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/0b2a9cd9990b/sciadv.adm9797-f6.jpg
PMC11095461,"At the same time, we were surprised to find that the two features that differed most between song and speech were not pitch stability and rhythmic regularity but rather pitch height and temporal rate (Fig. 4).",PMC11095461_figure_4,Fig. 4. Plot of effect sizes showing differences of each feature between singing and spoken description of the 73 recording sets for the confirmatory analysis and 75 recording sets for the exploratory analysis.,"The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",./data/PMC11095461/images/figure_4.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/a42b9222da86/sciadv.adm9797-f4.jpg
PMC11095461,"Since our main interest lies in the identification of which features demonstrate differences or similarities between song and speech, we perform the within-participants comparison of the six features between the pairs of singing and speech, using the spoken description rather than the lyric recitation as the proxy for speech (compare red boxes in Fig. 1; the comparisons with lyrics recitation and with instrumental versions are saved for exploratory analyses).",PMC11095461_figure_1,Fig. 1. Example excerpts of the four recording types collected in this study arranged in a musi-linguistic continuum from instrumental music to spoken language.,"Spectrograms [x axis, time (in seconds); y axis, frequency (in hertz)] of the four types of recordings are displayed on the right-hand side (excerpts of author Savage performing/describing “Twinkle Twinkle Little Star,” using a piano for the instrumental version). Blue dashed lines show the schematic illustration of the mapping between the audio signal and acoustic units (here syllables/notes). For this Registered Report, we focus our confirmatory hypothesis only on comparisons between singing and spoken description (red rectangles), with recited and instrumental versions saved for post hoc exploratory analysis.",./data/PMC11095461/images/figure_1.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/42dfde229482/sciadv.adm9797-f1.jpg
PMC11095461,"In addition, terms in the computed difference scores are arranged so that for our predicted differences (H1 to H3), a positive value indicates a difference in the predicted direction (compare Fig. 8).",PMC11095461_figure_8,Fig. 8. Schematic overview of the analysis pipeline from raw audio recordings to the paired comparisons.,"This illustration is based on the pilot analysis of stage 1 (fig. S2), which served as a foundation for the subsequent main confirmatory analysis (Fig. 4). Recording sets 1 and 2 represent pilot data of singing and speaking in Yoruba and Farsi by coauthors F.N. and S.H., respectively. From each pair of song/spoken audio recordings by a given person, we quantify the difference using the effect size for each feature. Pre is the relative effect (converted to Cohen’s D for ease of interpretability). In both cases, the distributions of sung and spoken pitch overlap slightly, but song is substantially higher on average (Cohen’s D > 2). To synthesize the effect sizes collected from each recording pair to test our hypotheses, we apply meta-analyses by treating each recording pair as a study. This approach allows us to make an inference about the population effect size of features in song and speech samples. This example focuses on just one feature (pitch height) applied to just two recording sets, but the same framework is applied to the other five features and all recording sets in the actual analysis. Different types of hypothesis testing are applied depending on the feature (i.e., hypothesis of difference and hypothesis of similarity).",./data/PMC11095461/images/figure_8.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/6af62ec04be7/sciadv.adm9797-f8.jpg
PMC11095461,We apply the meta-analysis framework to synthesize the effect size across recordings to make statistical inference for each hypothesis (Fig. 8).,PMC11095461_figure_8,Fig. 8. Schematic overview of the analysis pipeline from raw audio recordings to the paired comparisons.,"This illustration is based on the pilot analysis of stage 1 (fig. S2), which served as a foundation for the subsequent main confirmatory analysis (Fig. 4). Recording sets 1 and 2 represent pilot data of singing and speaking in Yoruba and Farsi by coauthors F.N. and S.H., respectively. From each pair of song/spoken audio recordings by a given person, we quantify the difference using the effect size for each feature. Pre is the relative effect (converted to Cohen’s D for ease of interpretability). In both cases, the distributions of sung and spoken pitch overlap slightly, but song is substantially higher on average (Cohen’s D > 2). To synthesize the effect sizes collected from each recording pair to test our hypotheses, we apply meta-analyses by treating each recording pair as a study. This approach allows us to make an inference about the population effect size of features in song and speech samples. This example focuses on just one feature (pitch height) applied to just two recording sets, but the same framework is applied to the other five features and all recording sets in the actual analysis. Different types of hypothesis testing are applied depending on the feature (i.e., hypothesis of difference and hypothesis of similarity).",./data/PMC11095461/images/figure_8.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/6af62ec04be7/sciadv.adm9797-f8.jpg
PMC11095461,Made Fig. 2 word clouds: J.S.G.-C.,PMC11095461_figure_2,Fig. 2. Visualization of the diversity of the primary sample of 300 audio recordings of singing/speaking/recitation/instrumental melodies.,"Map of the linguistic varieties spoken by our 75 coauthors as first/heritage languages (A). (Note: 6 of the original 81 planned coauthors were unable to complete the recording and annotation process compared to our initially planned sample; compare fig. S1 for the original map of 81 linguistic varieties). Each circle represents a coauthor singing and speaking in their first (L1) or heritage language. The geographic coordinates represent their hometown where they learned that language. In cases when the language name preferred by that coauthor (ethnonym) differs from the L1 language name in the standardized classification in the Glottolog (47), the ethnonym is listed first, followed by the Glottolog name in round brackets. Language family classifications (in bold) are based on Glottolog. Square brackets indicate geographic locations for languages represented by more than one coauthor. Atlantic-Congo, Indo-European, and Sino-Tibetan languages are further grouped by genus defined by the World Atlas of Language Structures (48). The word clouds outline the most common textual content of English translations of the song lyrics (B) and spoken descriptions (C) provided by our 75 coauthors (larger text indicates words that appear more frequently).",./data/PMC11095461/images/figure_2.jpg,https://cdn.ncbi.nlm.nih.gov/pmc/blobs/16c2/11095461/98aa95a54c26/sciadv.adm9797-f2.jpg
