{
  "figure_1": "Spectrograms [x axis, time (in seconds); y axis, frequency (in hertz)] of the four types of recordings are displayed on the right-hand side (excerpts of author Savage performing/describing “Twinkle Twinkle Little Star,” using a piano for the instrumental version). Blue dashed lines show the schematic illustration of the mapping between the audio signal and acoustic units (here syllables/notes). For this Registered Report, we focus our confirmatory hypothesis only on comparisons between singing and spoken description (red rectangles), with recited and instrumental versions saved for post hoc exploratory analysis.",
  "figure_2": "Map of the linguistic varieties spoken by our 75 coauthors as first/heritage languages (A). (Note: 6 of the original 81 planned coauthors were unable to complete the recording and annotation process compared to our initially planned sample; compare fig. S1 for the original map of 81 linguistic varieties). Each circle represents a coauthor singing and speaking in their first (L1) or heritage language. The geographic coordinates represent their hometown where they learned that language. In cases when the language name preferred by that coauthor (ethnonym) differs from the L1 language name in the standardized classification in the Glottolog (47), the ethnonym is listed first, followed by the Glottolog name in round brackets. Language family classifications (in bold) are based on Glottolog. Square brackets indicate geographic locations for languages represented by more than one coauthor. Atlantic-Congo, Indo-European, and Sino-Tibetan languages are further grouped by genus defined by the World Atlas of Language Structures (48). The word clouds outline the most common textual content of English translations of the song lyrics (B) and spoken descriptions (C) provided by our 75 coauthors (larger text indicates words that appear more frequently).",
  "figure_3": "Onset and breathing annotations are based on the segmented texts displayed on the top of the spectrogram. The y axis is adjusted to emphasize the f0 contour, so note that the spectral centroid information is not fully captured (e.g., high spectral centroid due to the consonant). The bottom figure shows pitch stability (rate of change of f0 or derivative of the f0 contour equivalently) of the sung f0.",
  "figure_4": "The plot includes seven additional exploratory features, and the six features corresponding to the main confirmatory hypotheses are enclosed by the red rectangle. Confidence intervals are created using the same criteria in the confirmatory analysis (i.e., α = 0.05/6). Each circle represents the effect size from each recording pair of singing and spoken description, and the set of effect sizes is measured per recording pair. Readers can find further information about how to interpret the figure in the caption of figs. S2 and S9. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2, and violin plots are added to this figure compared to fig. S2.",
  "figure_5": "This is an alternative visualization of the same sung/spoken data from Fig. 4, but showing mean values of each feature rather than paired differences, and now also including data for instrumental melodies and recited lyrics. The cent scale of f0 is converted from Hertz, where 440 Hz corresponds to 0 cents and an octave interval equals 1200 cents. Note that the colors of data points indicate language families, which are coded the same as in Fig. 2. The horizontal lines in the violin plots indicate the median.",
  "figure_6": "Three features could be directly compared between the different samples: pitch height (A), pitch stability (B), and timbral brightness (C). The following samples were compared: (i) Our full sample (matched song and speech recordings from our 75 coauthors); (ii) Hilton et al.’s (26) full sample (matched song and speech recordings from 209 individuals); (iii) a subsample of our 14 coauthors singing/speaking in English, Spanish, Mandarin, Kannada, and Polish; and (iv) a subsample of Hilton et al.’s (26) 122 participants also singing/speaking in English, Spanish, Mandarin, Kannada, and Polish). “SA” means that f0 values are extracted in a semiautomated manner (compare the “Pitch height” section in the Supplementary Materials), while “FA” means they were exactly in a fully automated manner (using the pYIN algorithm). The visualization follows the same convention as in Figs. 4 and 5. However, Hilton et al.’s (26) dataset contains languages that are not in our dataset. Therefore, slightly different color mapping was applied (compare fig. S16). Note that some large effect sizes (D > 3.5) in the pitch height of our original analysis (i.e., full, SA, 20 s) are not observed in the automated analysis (i.e., full, FA, full length). This is due to estimation errors in the automated analyses. When erroneous f0 values of pYIN are very high in spoken description or very low in singing, relative effects become smaller than semiautomated methods that remove these errors.",
  "figure_7": "f0 contours extracted by the segments between onset and break were averaged to visualize the overall trend. The length of contours is normalized to 128 samples. The average widths of confidence intervals of each category are 0.14 for instrumental, 0.097 for song, 0.060 for lyrics recitation, and 0.065 for spoken description. The details of the computation is provided in the “Computation of average f0 contours of Fig. 7” section in the Supplementary Materials.",
  "figure_8": "This illustration is based on the pilot analysis of stage 1 (fig. S2), which served as a foundation for the subsequent main confirmatory analysis (Fig. 4). Recording sets 1 and 2 represent pilot data of singing and speaking in Yoruba and Farsi by coauthors F.N. and S.H., respectively. From each pair of song/spoken audio recordings by a given person, we quantify the difference using the effect size for each feature. Pre is the relative effect (converted to Cohen’s D for ease of interpretability). In both cases, the distributions of sung and spoken pitch overlap slightly, but song is substantially higher on average (Cohen’s D > 2). To synthesize the effect sizes collected from each recording pair to test our hypotheses, we apply meta-analyses by treating each recording pair as a study. This approach allows us to make an inference about the population effect size of features in song and speech samples. This example focuses on just one feature (pitch height) applied to just two recording sets, but the same framework is applied to the other five features and all recording sets in the actual analysis. Different types of hypothesis testing are applied depending on the feature (i.e., hypothesis of difference and hypothesis of similarity)."
}