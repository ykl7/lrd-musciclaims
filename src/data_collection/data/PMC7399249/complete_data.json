{
  "paper_id": "PMC7399249",
  "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7399249/",
  "figures": {
    "figure_1": {
      "figure_number": "Figure 1",
      "title": "FIGURE 1.",
      "caption": "Techniques of lighting and viewpoint capture used in highly cited 3D + 4D facial databases. See comprehensive review of capture techniques by Sandbach et al. (2012) and Table 1 in Zollhöfer et al. (2018).",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F1",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_1.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/6bef9a5f7f0f/fpsyg-11-01842-g001.jpg"
    },
    "figure_2": {
      "figure_number": "Figure 2",
      "title": "FIGURE 2.",
      "caption": "2D image compared to the 4D video-frame of a highly cited 4D facial database (The images pictured above were reproduced, modified and adapted from the facial database originally developed by Zhang et al. (2013; 2014). Copyright 2013–2017, The Research Foundation for the State University of New York and University of Pittsburgh of the Commonwealth System of Higher Education. All Right Reserved. BP4D-Spontaneous Database, as of: 26/03/2018. (A) A flattened face. Traditional camera set-up produces a 2D image or photograph in gray-scale. (B) A bird’s eye view of 3D geometric mesh. Multi-view camera set-up producing a 3D structured facial mesh, with skin texture, using 30,000–50,000 vertices (viewed from top of skull). (C) A 3D sense of re-created depth. Despite being on a two-dimensional computer screen, a 3D face reconstructed with multi-view camera set-up, produced an illusion of depth. The final reconstruction produces the same facial model, which stands out from the background and appear more lifelike than the 2D photograph. When working with 3D facial stimuli, we can change our viewpoint of the facial stimulus. Here we have manipulated the viewer angle of the camera along a Z-axis is possible (top-frontal view), to view the same frame of a face from many possible 360 degree viewpoints.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F2",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_2.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/96d6b3d00799/fpsyg-11-01842-g002.jpg"
    },
    "figure_3": {
      "figure_number": "Figure 3",
      "title": "FIGURE 3.",
      "caption": "3D and 4D facial databases publication frequency across Google Scholar, Scopus and Web of Science. Search on 13/02/2020, including count of the original published journal article of each database.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F3",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_3.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/5d53e60a5534/fpsyg-11-01842-g003.jpg"
    },
    "figure_4": {
      "figure_number": "Figure 4",
      "title": "FIGURE 4.",
      "caption": "Logarithmic Scale of Scopus publication growth from 1980 to 2019 across fields containing key search terms. Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F4",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_4.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/2a2a836f51bd/fpsyg-11-01842-g004.jpg"
    },
    "figure_5": {
      "figure_number": "Figure 5",
      "title": "FIGURE 5.",
      "caption": "BP-4D-S facial database presented on gray background with bounding box representing a 3D depth plane (The images pictured above were reproduced, modified and adapted from the facial database originally developed by Zhang et al. (2013, 2014). Copyright 2013–2017, The Research Foundation for the State University of New York and University of Pittsburgh of the Commonwealth System of Higher Education. All Right Reserved. BP4D-Spontaneous Database, as of: 26/03/2018. 3D faces are built on-screen as models with a geometric mesh in a three-dimensional plane; presenting an anatomically accurate modeled object with width, height and depth. This is captured by the 3D camera set-up. This reconstruction by computer-scientists is highly individualized, but is typically underlined by a geometric mesh or structure. The 3D surface features of the individual are then rendered. As the face moves around in 3D space, it becomes apparent why having three dimensions maintains facial structure more realistically than a facial model built with only two dimensions, as indicated by the depth plane.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F5",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_5.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/f06559fa6b36/fpsyg-11-01842-g005.jpg"
    },
    "figure_6": {
      "figure_number": "Figure 6",
      "title": "FIGURE 6.",
      "caption": "Changing facial viewpoint along a 3D axis with upper and lower head tilts (The images pictured above were reproduced, modified and adapted from the facial database originally developed by Zhang et al. (2013, 2014). Copyright 2013–2017, The Research Foundation for the State University of New York and University of Pittsburgh of the Commonwealth System of Higher Education. All Right Reserved. BP4D-Spontaneous Database, as of: 26/03/2018.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F6",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_6.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/b5b4cc4fa385/fpsyg-11-01842-g006.jpg"
    },
    "figure_7": {
      "figure_number": "Figure 7",
      "title": "FIGURE 7.",
      "caption": "Video-recorded dynamic 4D facial stimulus captured as still frame-by-frame images, displaying happy and disgusted FACS coded emotion (The images pictured above were reproduced, modified and adapted from the facial database originally developed by Zhang et al. (2013, 2014). Copyright 2013–2017, The Research Foundation for the State University of New York and University of Pittsburgh of the Commonwealth System of Higher Education. All Right Reserved. BP4D-Spontaneous Database, as of: 26/03/2018. This exemplar presents the frame-by-frame stills from a video-recording of a 4D face, moving from a happy expression to a disgusted expression.",
      "possible_urls": [
        [
          "original_src",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg"
        ],
        [
          "large_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg?maxwidth=2000"
        ],
        [
          "original_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg?size=original"
        ],
        [
          "xlarge_size",
          "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg?maxwidth=4000"
        ]
      ],
      "figure_id": "F7",
      "is_table": false,
      "working_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg",
      "download_success": true,
      "local_image_path": "./data/PMC7399249/images/figure_7.jpg",
      "final_url": "https://cdn.ncbi.nlm.nih.gov/pmc/blobs/8d7d/7399249/29cd04a6b280/fpsyg-11-01842-g007.jpg"
    }
  },
  "claims": [
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "For a more detailed overview of how facial information is captured with lighting or viewpoint camera arrays within these databases, see Figure 1.",
      "figure_references": [
        {
          "figure_number": "Figure 1",
          "panel": "",
          "figure_key": "figure_1"
        }
      ]
    },
    {
      "sentence": "For example, the database presented in Figure 2B, experiences a loss of data from the posterior viewpoint of the head, which may impede studies examining VR environments (Bulthoff and Edelman, 1992; Stolz et al., 2019; Lamberti et al., 2020) or multiple viewpoints of a facial stimulus (Bülthoff et al., 2018; Abudarham et al., 2019; Zhan et al., 2019).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "Panel B",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Overall, despite some remaining challenges, 3D and 4D facial capture and reconstruction has demonstrated that the unique facial features belonging to you and me, such as the cheeks, eyes and nose, can simulated on-screen in a highly realistic manner (see Figure 2).",
      "figure_references": [
        {
          "figure_number": "Figure 2",
          "panel": "",
          "figure_key": "figure_2"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "Search on 13/02/2020, including count of the original published journal article of each database.At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Search terms: (“allintitle, abstract or keywords”: “3-D face” OR “3D face” OR “three-dimensional face” OR “three-dimensional face” OR “4D”).FIGURE 5.",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "Similarly, the dynamic video-recorded BU4D-S 4D database produced by Zhang et al. (2013, 2014) is the leading 4D database in citation count (Figure 3).",
      "figure_references": [
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        },
        {
          "figure_number": "Figure 3",
          "panel": "",
          "figure_key": "figure_3"
        }
      ]
    },
    {
      "sentence": "At first glance, the citation count pictured in Figure 4 appears impressive.",
      "figure_references": [
        {
          "figure_number": "Figure 4",
          "panel": "",
          "figure_key": "figure_4"
        }
      ]
    },
    {
      "sentence": "Thus we examined logarithmic growth across academic fields below (Figure 5).",
      "figure_references": [
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        },
        {
          "figure_number": "Figure 5",
          "panel": "",
          "figure_key": "figure_5"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "One of the greatest contributions 3D face databases offer is the inclusion of unconstrained head motion within space (for some examples see Figure 6 MPI; Kaulard et al., 2012; BP-4D-S; Zhang et al., 2014).",
      "figure_references": [
        {
          "figure_number": "Figure 6",
          "panel": "",
          "figure_key": "figure_6"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    },
    {
      "sentence": "Exemplars of different FACS coded muscle activations, illustrating smiles, frowns and grimaces of deformable facial features are displayed in Figure 7.",
      "figure_references": [
        {
          "figure_number": "Figure 7",
          "panel": "",
          "figure_key": "figure_7"
        }
      ]
    }
  ],
  "extraction_stats": {
    "figures_count": 7,
    "claims_count": 91,
    "images_downloaded": 7,
    "tables_filtered": 250
  }
}