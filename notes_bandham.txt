09/22/2025:

https://pubmed.ncbi.nlm.nih.gov/help/#advanced-search
https://pubmed.ncbi.nlm.nih.gov/help/#finding-full-text
https://pubmed.ncbi.nlm.nih.gov/help/#pt
https://pubmed.ncbi.nlm.nih.gov/?term=covid-19+NOT+preprint%5Bpt%5D

- Take only papers that are only accepted.

reviewed and good journals.

not older than 2015.

Search will have publication date - make sure to use it.

use search api for best use.


10/06/2025:
Qwen 8B
- for small model the prompts has to be consice
- look at musciclaim section: pertrunb 3: there is a prompt.
- musclaim github - human annotation app: read instructions: look at interannotatordata.py
    - it verifies the claim: valid perturbation or not.
    - only new change is generation, not annotation
    - the differentiating condition is mentioned in the file.
- 500 data points: contradictions.: by by qwen model
- 500: nuertral datapoints by swapping firgure names
- we need to finetune it: vlm finetuning: vl2.5
- accuracy test is: small subset that doest go into training - run inference on that with trained model - see what performance.
- Get the total claims number and share it with yash.


10/16/2025:
    - st.text(f”You are given a claim and its perturbation. Your task is to judge whether the perturbation is a contradiction of the original claim. Both the original and perturbed claim cannot be true at the same time.“)
    - “Your task is to perturb the given original claim and produce a contradiction of the original claim. Both the original and perturbed claim cannot be true at the same time.”
    - manually review atleast 50 claims with figures as reference.
    - prepare llm judge for validating perturbated and original claim same as what .py is doing (yash).
    - remove figure reference from the claim text.
    - after judge, we will have two data sets - one all data points (s + p + n) & filtered data with judge.


DID: Used LLM (Qwen8B) for perturbation, for removing fig reference, for judging if the perturbated & original are valid or not.

- Clear the figure reference from the given claim. without changing the meaning at any cost. 

10/15/2025: Part 1 (Generating contradict & neutral data using Qwen)

# Command to start running vllm in the background
nohup python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-VL-8B-Instruct --host 0.0.0.0 --port 8002 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 8192 > ~/vllm.log 2>&1 & tail -f ~/vllm.log

# comment to stay connected to cluster in the background.
ssh -fN context3



10/22/2025:
- give ip, fig, claim and caption, train to produce judgement. Just jdgement
- panel + judgement
- read / explore panel segmentation. image based segmentation. try to talk to people about this. People doing research.

do we fine-tune the model? Lora, 32 16.

train & dev set (10%)
at end acc should go up and during training loss should go down.
decode and get the accuracy.
also read about patience / early stopping parameter for this fine-tuning.




I want to fine-tune a Qwen 8B model by passing panels, fig file, claim and caption, train to produce judgement that SUPPORT CONTRADICT OR NEUTRAL. Just jdgement, no reasoning.
all the above data fields are columns.
- train & dev set (10%)

Lora, 32 x 16.
Accuracy is the validation metric.
decode the model outputs to actual label and get the accuracy.
use patience / early stopping parameter for this fine-tuning.




ssh -L 8003:a100-01:8003 mbandham@dn-rome1 -N


srun --partition=a100-long --nodelist=a100-06 --gres=gpu:2 --mem=200G --time=48:00:00 --pty bash


nohup python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-VL-8B-Instruct --host 0.0.0.0 --port 8002 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 8192 > ~/vllm.log 2>&1 & tail -f ~/vllm.log

Order:
perbate .py
combine datasets .py
extract panel names .py
remove fig ref .py


nohup python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-VL-8B-Instruct --host 0.0.0.0 --port 8003 --gpu-memory-utilization 0.9 --tensor-parallel-size 1 --max-model-len 8192 > ~/vllm.log 2>&1 & tail -f ~/vllm.log

12192




